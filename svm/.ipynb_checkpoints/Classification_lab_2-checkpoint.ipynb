{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2: predicting data_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter('ignore',DeprecationWarning)\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "data_file = 'OnlineNewsPopularity.csv'\n",
    "\n",
    "file_2_read = data_dir + data_file\n",
    "df = pd.read_csv(file_2_read)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "col_names = df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df['n_non_stop_words']\n",
    "del df['n_non_stop_unique_tokens']\n",
    "del df['n_unique_tokens']\n",
    "del df['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ... creating data_channel categorical variable\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "df['data_channel'] = 'Others'\n",
    "\n",
    "condition = df['data_channel_is_lifestyle'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'Lifestyle'\n",
    "\n",
    "condition = df['data_channel_is_entertainment'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'Entertainment'\n",
    "\n",
    "condition = df['data_channel_is_bus'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'Business'\n",
    "\n",
    "condition = df['data_channel_is_socmed'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'Social Media'\n",
    "\n",
    "condition = df['data_channel_is_tech'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'Technology'\n",
    "\n",
    "condition = df['data_channel_is_world'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'World'\n",
    "\n",
    "del df['data_channel_is_lifestyle']\n",
    "del df['data_channel_is_entertainment']\n",
    "del df['data_channel_is_bus']\n",
    "del df['data_channel_is_socmed']\n",
    "del df['data_channel_is_tech']\n",
    "del df['data_channel_is_world']\n",
    "del df['shares'] \n",
    "#shares is our task 1 dependent variable, we are excluding it from this model as per business model this value is not available\n",
    "# during data_channel prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  convert the data type to Integer\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "to_int = ['timedelta','n_tokens_title', 'n_tokens_content',\n",
    "    'num_hrefs','num_self_hrefs', 'num_imgs', 'num_videos', 'num_keywords',\n",
    "    'weekday_is_monday',\n",
    "    'weekday_is_tuesday',\n",
    "    'weekday_is_wednesday',\n",
    "    'weekday_is_thursday',\n",
    "    'weekday_is_friday',\n",
    "    'weekday_is_saturday',\n",
    "    'weekday_is_sunday',\n",
    "    'is_weekend']\n",
    "    \n",
    "\n",
    "df[to_int] = df[to_int ].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>data_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timedelta, n_tokens_title, n_tokens_content, num_hrefs, num_self_hrefs, num_imgs, num_videos, average_token_length, num_keywords, kw_min_min, kw_max_min, kw_avg_min, kw_min_max, kw_max_max, kw_avg_max, kw_min_avg, kw_max_avg, kw_avg_avg, self_reference_min_shares, self_reference_max_shares, self_reference_avg_sharess, weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, weekday_is_thursday, weekday_is_friday, weekday_is_saturday, weekday_is_sunday, is_weekend, LDA_00, LDA_01, LDA_02, LDA_03, LDA_04, global_subjectivity, global_sentiment_polarity, global_rate_positive_words, global_rate_negative_words, rate_positive_words, rate_negative_words, avg_positive_polarity, min_positive_polarity, max_positive_polarity, avg_negative_polarity, min_negative_polarity, max_negative_polarity, title_subjectivity, title_sentiment_polarity, abs_title_subjectivity, abs_title_sentiment_polarity, data_channel]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 51 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_content 2.94542193879 ln_n_tokens_content\n",
      "num_hrefs 4.0134948282 ln_num_hrefs\n",
      "num_self_hrefs 5.17275110576 ln_num_self_hrefs\n",
      "num_imgs 3.94659584465 ln_num_imgs\n",
      "num_videos 7.0195327863 ln_num_videos\n",
      "kw_min_min 2.37494728018 ln_kw_min_min\n",
      "--> Ln() transform not completed -- skew > 1, but min value < 0 : kw_min_min !!\n",
      "kw_max_min 35.3284337312 ln_kw_max_min\n",
      "kw_avg_min 31.3061081027 ln_kw_avg_min\n",
      "--> Ln() transform not completed -- skew > 1, but min value < 0 : kw_avg_min !!\n",
      "kw_min_max 10.3863716348 ln_kw_min_max\n",
      "kw_max_avg 16.4116695554 ln_kw_max_avg\n",
      "kw_avg_avg 5.76017729162 ln_kw_avg_avg\n",
      "self_reference_min_shares 26.2643641603 ln_self_reference_min_shares\n",
      "self_reference_max_shares 13.8708490494 ln_self_reference_max_shares\n",
      "self_reference_avg_sharess 17.9140933777 ln_self_reference_avg_sharess\n",
      "weekday_is_monday 1.77590824423 ln_weekday_is_monday\n",
      "weekday_is_tuesday 1.61054706191 ln_weekday_is_tuesday\n",
      "weekday_is_wednesday 1.60097097689 ln_weekday_is_wednesday\n",
      "weekday_is_thursday 1.6370700483 ln_weekday_is_thursday\n",
      "weekday_is_friday 2.03030483518 ln_weekday_is_friday\n",
      "weekday_is_saturday 3.63708575997 ln_weekday_is_saturday\n",
      "weekday_is_sunday 3.3999273763 ln_weekday_is_sunday\n",
      "is_weekend 2.18850033431 ln_is_weekend\n",
      "LDA_00 1.5674632332 ln_LDA_00\n",
      "LDA_01 2.08672182342 ln_LDA_01\n",
      "LDA_02 1.31169490203 ln_LDA_02\n",
      "LDA_03 1.23871598638 ln_LDA_03\n",
      "LDA_04 1.17312947598 ln_LDA_04\n",
      "global_rate_negative_words 1.49191730919 ln_global_rate_negative_words\n",
      "min_positive_polarity 3.04046773746 ln_min_positive_polarity\n",
      "abs_title_sentiment_polarity 1.70419343991 ln_abs_title_sentiment_polarity\n",
      "['n_tokens_content', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'kw_max_min', 'kw_min_max', 'kw_max_avg', 'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_rate_negative_words', 'min_positive_polarity', 'abs_title_sentiment_polarity']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-3437606373eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[0mcolumns_to_drop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weekday_is_saturday'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[0mcolumns_to_drop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weekday_is_sunday'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[0mcolumns_to_drop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_channel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  ln() transform right skewed distribution variables (skewness > 1)\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "df_numeric = df.select_dtypes(['number'])\n",
    "\n",
    "numeric_col_names = df_numeric.columns.values.tolist()\n",
    "\n",
    "# ... store min value for each column\n",
    "\n",
    "df_mins = df.min()\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  loop on each column, test for skewness, create new column if conditions met\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "columns_to_drop = []\n",
    "\n",
    "for column in numeric_col_names:\n",
    "    sk = df[column].skew()\n",
    "    \n",
    "    if(sk > 1):\n",
    "        new_col_name = 'ln_' + column\n",
    "        print (column, sk, new_col_name)\n",
    "        \n",
    "        if df_mins[column] > 0:\n",
    "            df[new_col_name] = np.log(df[column])\n",
    "            columns_to_drop.append(column)\n",
    "            \n",
    "        elif df_mins[column] == 0:\n",
    "            df_tmp = df[column] + 1\n",
    "            df[new_col_name] = np.log(df_tmp)\n",
    "            columns_to_drop.append(column)\n",
    "            \n",
    "        else:\n",
    "            print('--> Ln() transform not completed -- skew > 1, but min value < 0 :', column, '!!')\n",
    "            \n",
    "            \n",
    "# ... delete tmp data\n",
    "\n",
    "del df_tmp\n",
    "del df_mins\n",
    "del df_numeric\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  based on inspection, a few of these are just not valid ranges in ln() space\n",
    "# ...  -- just delete these few back out of the data set\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "print (columns_to_drop)\n",
    "\n",
    "\n",
    "\n",
    "del df['ln_LDA_00']\n",
    "del df['ln_LDA_01']\n",
    "del df['ln_LDA_02']\n",
    "del df['ln_LDA_03']\n",
    "del df['ln_LDA_04']\n",
    "columns_to_drop.remove('LDA_00')\n",
    "columns_to_drop.remove('LDA_01')\n",
    "columns_to_drop.remove('LDA_02')\n",
    "columns_to_drop.remove('LDA_03')\n",
    "columns_to_drop.remove('LDA_04')\n",
    "\n",
    "\n",
    "# ...  these are binary indicators ... so no need to ln-transform\n",
    "\n",
    "del df['ln_weekday_is_monday']\n",
    "del df['ln_weekday_is_tuesday']\n",
    "del df['ln_weekday_is_wednesday']\n",
    "del df['ln_weekday_is_thursday']\n",
    "del df['ln_weekday_is_friday']\n",
    "del df['ln_weekday_is_saturday']\n",
    "del df['ln_weekday_is_sunday']\n",
    "del df['ln_is_weekend']\n",
    "columns_to_drop.remove('is_weekend')\n",
    "columns_to_drop.remove('weekday_is_monday')\n",
    "columns_to_drop.remove('weekday_is_tuesday')\n",
    "columns_to_drop.remove('weekday_is_wednesday')\n",
    "columns_to_drop.remove('weekday_is_thursday')\n",
    "columns_to_drop.remove('weekday_is_friday')\n",
    "columns_to_drop.remove('weekday_is_saturday')\n",
    "columns_to_drop.remove('weekday_is_sunday')\n",
    "columns_to_drop.remove('data_channel')\n",
    "\n",
    "\n",
    "\n",
    "# ... not needed for binary analysis ... will use popular indicator\n",
    "\n",
    "del df['ln_shares'] \n",
    "columns_to_drop.remove('shares')\n",
    "\n",
    "print ('\\n-----------------------------------\\n')\n",
    "print ('Number of current columns in dataset :', len(df.columns))\n",
    "\n",
    "\n",
    "\n",
    "df.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #TODO: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timedelta', 'n_tokens_title', 'n_tokens_content', 'num_hrefs',\n",
       "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
       "       'num_keywords', 'kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max',\n",
       "       'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'kw_avg_avg',\n",
       "       'self_reference_min_shares', 'self_reference_max_shares',\n",
       "       'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday',\n",
       "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
       "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00',\n",
       "       'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
       "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
       "       'global_rate_negative_words', 'rate_positive_words',\n",
       "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
       "       'max_positive_polarity', 'avg_negative_polarity',\n",
       "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
       "       'abs_title_sentiment_polarity', 'data_channel', 'ln_n_tokens_content',\n",
       "       'ln_num_hrefs', 'ln_num_self_hrefs', 'ln_num_imgs', 'ln_num_videos',\n",
       "       'ln_kw_max_min', 'ln_kw_min_max', 'ln_kw_max_avg', 'ln_kw_avg_avg',\n",
       "       'ln_self_reference_min_shares', 'ln_self_reference_max_shares',\n",
       "       'ln_self_reference_avg_sharess', 'ln_global_rate_negative_words',\n",
       "       'ln_min_positive_polarity', 'ln_abs_title_sentiment_polarity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits = num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'data_channel' in df:\n",
    "    y = df['data_channel'].values         # set 'popular' as dependent\n",
    "    del df['data_channel']                # remove from dataset\n",
    "    X = df.values     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  run through the cross validation loop and set the training and testing\n",
    "# ...  variable for one single iteration\n",
    "# ...\n",
    "# ...  --> this method is memory-user, but easier to follow what is being done \n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X, y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ... scale attributes by the training set\n",
    "# ... - normalize features based on mean & stdev of each column\n",
    "# ... - do not use the testing data\n",
    "# ... - use what was last stored in the variables: X_train, y_train, X_test, y_test\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)                        # scale for each column for (0,1) mean, std\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comparison_tbl = pd.DataFrame(columns = ['Model Name','Accuracy','Processing Time'])\n",
    "i_index=[]\n",
    "i_index = 0\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import confusion_matrix as conf\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from tabulate import tabulate\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score,accuracy_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 : Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7765\n",
      "confusion matrix\n",
      " [[1046   12   40  107   44   15   13]\n",
      " [  10 1122   25   39   16   32  179]\n",
      " [  21    7  225   32   99    7   15]\n",
      " [  64   23   59  247   16   31   23]\n",
      " [  27   23  236   41 1007   59    2]\n",
      " [  23   40   42   62   55 1458   28]\n",
      " [   5  129   32   24    6    9 1052]]\n",
      "process time 2.2104\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tic = time.clock()\n",
    "\n",
    "#basic multiclass LR\n",
    "lr_model1 = LogisticRegression(class_weight='balanced', multi_class='multinomial', solver='lbfgs')\n",
    "lr_model1.fit(X_train_scaled, y_train)  # train object\n",
    "y_hat = lr_model1.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "\n",
    "accuracy = '{0:.4f}'.format(metrics.accuracy_score(y_test, y_hat))\n",
    "print(\"accuracy\",accuracy )\n",
    "print(\"confusion matrix\\n\", conf(y_test, y_hat))\n",
    "toc =  time.clock()\n",
    "exetime = '{0:.4f}'.format(toc-tic)\n",
    "print('process time',exetime)\n",
    "print(\"\\n\")\n",
    "\n",
    "raw_data = {\n",
    "    'Model Name':'Multinomial logistic regression',\n",
    "    'Accuracy':accuracy,\n",
    "    'Processing Time': exetime\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(raw_data,columns = ['Model Name','Accuracy','Processing Time'],index=[i_index+1])\n",
    "comparison_tbl = comparison_tbl.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.7711\n",
      "confusion matrix\n",
      " [[1030   16   45   97   48   21   13]\n",
      " [   8 1102   30   55   19   31  159]\n",
      " [  22   13  258   27   84    7   21]\n",
      " [  74   23   44  236   15   26   27]\n",
      " [  32   24  242   47 1011   72    3]\n",
      " [  17   40   40   67   59 1453   31]\n",
      " [  10  107   35   37   12   15 1024]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.7721\n",
      "confusion matrix\n",
      " [[ 991   16   33  104   52   20   12]\n",
      " [  15 1122   22   50   11   28  158]\n",
      " [  29    5  242   30   89   13   16]\n",
      " [  73   42   36  259   19   29   32]\n",
      " [  34   12  234   48 1063   74    3]\n",
      " [  14   37   46   67   56 1439   25]\n",
      " [   9  138   39   20    4   13 1006]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.7789\n",
      "confusion matrix\n",
      " [[1004   22   39  112   62   24   10]\n",
      " [  11 1123   21   36   16   45  173]\n",
      " [  16    3  258   31   77    6   22]\n",
      " [  57   37   20  278   17   35   25]\n",
      " [  37   24  253   42 1060   57    4]\n",
      " [  16   31   42   52   63 1378   18]\n",
      " [   8  121   29   22    5   12 1075]]\n",
      "====Iteration 3  ====\n",
      "accuracy 0.7725\n",
      "confusion matrix\n",
      " [[ 987   15   38   95   51   32   17]\n",
      " [  10 1099   28   58   20   30  180]\n",
      " [  20    4  252   29   86   11   17]\n",
      " [  60   28   42  269   18   26   31]\n",
      " [  34   21  248   47 1068   68    2]\n",
      " [  16   33   42   56   53 1381   31]\n",
      " [   9  109   45   29    5   10 1069]]\n",
      "====Iteration 4  ====\n",
      "accuracy 0.7674\n",
      "confusion matrix\n",
      " [[1043   18   38   97   49   19   13]\n",
      " [  13 1138   28   50   17   31  173]\n",
      " [  25    3  248   32   72    8   26]\n",
      " [  75   30   40  223   19   25   25]\n",
      " [  40   16  277   52 1001   75    2]\n",
      " [  15   46   40   69   68 1341   23]\n",
      " [   9  108   25   29    9   15 1091]]\n",
      "====Iteration 5  ====\n",
      "accuracy 0.7698\n",
      "confusion matrix\n",
      " [[1046   15   46  105   46   17   22]\n",
      " [  15 1124   28   50    5   34  165]\n",
      " [  19    6  245   31   75    8   20]\n",
      " [  63   35   49  249   14   30   28]\n",
      " [  41   22  236   49 1051   66    5]\n",
      " [  14   48   38   59   64 1385   30]\n",
      " [   7  134   33   29    7   17 1004]]\n",
      "====Iteration 6  ====\n",
      "accuracy 0.7720\n",
      "confusion matrix\n",
      " [[1036   23   46  104   50   15    9]\n",
      " [  12 1096   26   54   13   28  155]\n",
      " [  20    9  242   31  100    7   16]\n",
      " [  52   32   45  262   17   25   26]\n",
      " [  29   21  244   50 1042   69    1]\n",
      " [  22   35   39   71   56 1402   30]\n",
      " [   8  130   41   29    7   11 1041]]\n",
      "====Iteration 7  ====\n",
      "accuracy 0.7669\n",
      "confusion matrix\n",
      " [[1024   14   41  104   32   16    9]\n",
      " [  10 1072   24   47   11   36  180]\n",
      " [  24    6  244   36  105   13   16]\n",
      " [  67   33   33  257   20   30   21]\n",
      " [  53   29  265   36 1039   66    5]\n",
      " [  17   45   36   58   71 1431   32]\n",
      " [  11  119   32   24    4   17 1014]]\n",
      "====Iteration 8  ====\n",
      "accuracy 0.7719\n",
      "confusion matrix\n",
      " [[1013   20   31   99   43   24   17]\n",
      " [  10 1112   27   54   20   30  152]\n",
      " [  27    5  229   45   89   14   20]\n",
      " [  64   30   32  262   12   28   23]\n",
      " [  35   14  253   58 1050   69    1]\n",
      " [  21   40   28   65   59 1443   20]\n",
      " [   6  120   43   39    9   13 1011]]\n",
      "====Iteration 9  ====\n",
      "accuracy 0.7765\n",
      "confusion matrix\n",
      " [[1046   12   40  107   44   15   13]\n",
      " [  10 1122   25   39   16   32  179]\n",
      " [  21    7  225   32   99    7   15]\n",
      " [  64   23   59  247   16   31   23]\n",
      " [  27   23  236   41 1007   59    2]\n",
      " [  23   40   42   62   55 1458   28]\n",
      " [   5  129   32   24    6    9 1052]]\n"
     ]
    }
   ],
   "source": [
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits = num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "cv_accuracy=[]\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    scl_obj = StandardScaler()\n",
    "    scl_obj.fit(X_train)                        # scale for each column for (0,1) mean, std\n",
    "    \n",
    "    X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "    X_test_scaled = scl_obj.transform(X_test) \n",
    "    \n",
    "    lr_model1.fit(X_train_scaled, y_train)  # train object\n",
    "    y_hat = lr_model1.predict(X_test_scaled) # get test set precitions\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    accuracy = '{0:.4f}'.format(metrics.accuracy_score(y_test, y_hat))\n",
    "    print(\"accuracy\", accuracy)\n",
    "    print(\"confusion matrix\\n\", conf(y_test, y_hat))\n",
    "    cv_accuracy.append(accuracy)\n",
    "    iter_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.7711',\n",
       " '0.7721',\n",
       " '0.7789',\n",
       " '0.7725',\n",
       " '0.7674',\n",
       " '0.7698',\n",
       " '0.7720',\n",
       " '0.7669',\n",
       " '0.7719',\n",
       " '0.7765']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.743599445075\n",
      "process time 2.9764\n",
      "0.743599445075\n",
      "[[1056   33   31   69   56   27   18]\n",
      " [  29 1066   10   43   34   59  172]\n",
      " [  33   16  159   33  149   20   19]\n",
      " [  62   37   34  196   40   44   27]\n",
      " [  50   19  176   43 1112   86    8]\n",
      " [  46   57   11   55   85 1341   25]\n",
      " [  15  151   29   23   23   36  966]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "tic = time.clock()\n",
    "\n",
    "# train and fit\n",
    "DTclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "DTclassifier.fit(X_train, y_train)\n",
    "y_predDT = DTclassifier.predict(X_test)\n",
    "\n",
    "#accuracy\n",
    "acc = accuracy_score(y_test, y_predDT)\n",
    "print(acc)\n",
    "toc =  time.clock()\n",
    "exetime = '{0:.4f}'.format(toc-tic)\n",
    "print('process time',exetime)\n",
    "\n",
    "#used later in the code for comparison\n",
    "raw_data = {\n",
    "    'Model Name':'Decision Tree Classifier',\n",
    "    'Accuracy':accuracy,\n",
    "    'Processing Time': exetime\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(raw_data,columns = ['Model Name','Accuracy','Processing Time'],index=[i_index+1])\n",
    "comparison_tbl = comparison_tbl.append(df)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "cmDT = confusion_matrix(y_test, y_predDT)\n",
    "print(cmDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.799470298903\n",
      "process time 1.5112\n",
      "[[1169   21   10   16   47   20    7]\n",
      " [  31 1158    4    8   23   29  160]\n",
      " [  57   13  100   23  197   13   26]\n",
      " [ 103   42   11  162   42   47   33]\n",
      " [  61   26   69    8 1268   57    5]\n",
      " [  37   43    8   13   58 1435   26]\n",
      " [  13  131    9    7   11   25 1047]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "tic = time.clock()\n",
    "# train and test \n",
    "RFclf = RandomForestClassifier(criterion = 'entropy', max_depth=50, n_estimators=10, n_jobs=-1)\n",
    "RFclf.fit(X_train, y_train)\n",
    "y_predRF = RFclf.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "total_accuracyRF = mt.accuracy_score(y_test, y_predRF)\n",
    "print ('Accuracy', total_accuracyRF)\n",
    "toc =  time.clock()\n",
    "exetime = '{0:.4f}'.format(toc-tic)\n",
    "print('process time',exetime)\n",
    "\n",
    "#used later in the code for comparison\n",
    "raw_data = {\n",
    "    'Model Name':'Random Forest Classifier',\n",
    "    'Accuracy':total_accuracyRF,\n",
    "    'Processing Time': exetime\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(raw_data,columns = ['Model Name','Accuracy','Processing Time'],index=[i_index+1])\n",
    "comparison_tbl = comparison_tbl.append(df)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "cmRF2 = confusion_matrix(y_test, y_predRF)\n",
    "print(cmRF2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD3CAYAAADSftWOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a415123be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (RFclf)\n",
    "\n",
    "plt.barh(range(len(RFclf.feature_importances_)), RFclf.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Processing Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multinomial logistic regression</td>\n",
       "      <td>0.7686</td>\n",
       "      <td>2.4073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multinomial logistic regression</td>\n",
       "      <td>0.7686</td>\n",
       "      <td>2.7972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7686</td>\n",
       "      <td>2.8382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.79947</td>\n",
       "      <td>1.5112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model Name Accuracy Processing Time\n",
       "0  Multinomial logistic regression   0.7686          2.4073\n",
       "1  Multinomial logistic regression   0.7686          2.7972\n",
       "2         Decision Tree Classifier   0.7686          2.8382\n",
       "3         Random Forest Classifier  0.79947          1.5112"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_tbl = comparison_tbl.reset_index(drop=True)\n",
    "comparison_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3: Naive Bayes ***\n",
    "##  this part giving import error. need to be looked into\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_mnb = MultinomialNB(alpha=1.0)\n",
    "\n",
    "for a in (0.001,0.1,0.2,0.6,0.8,1.0):\n",
    "    clf_mnb = MultinomialNB(alpha=a)\n",
    "    clf_mnb.fit(X_train, y_train)\n",
    "    y_hatm = clf_mnb.predict(X_test)\n",
    "    accm = accuracy_score(y_test, y_hatm)\n",
    "    print('Multinomial Accuracy with alpha %.3f is: %.4f' %(a,accm)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'C': 100, 'tol': 0.001}\n",
      "-1.687 (+/-0.007) for {'C': 100, 'tol': 0.001}\n",
      "[-1.68430784 -1.69216796 -1.67386613 -1.69273491 -1.68946426]\n",
      "-1.687 (+/-0.007) for {'C': 100, 'tol': 0.0001}\n",
      "[-1.68430784 -1.69216796 -1.67386613 -1.69273491 -1.68946426]\n",
      "-1.687 (+/-0.007) for {'C': 1000, 'tol': 0.001}\n",
      "[-1.68430784 -1.69216787 -1.67389842 -1.69273463 -1.68946425]\n",
      "-1.687 (+/-0.007) for {'C': 1000, 'tol': 0.0001}\n",
      "[-1.68430784 -1.69216787 -1.67389842 -1.69273463 -1.68946425]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "params = {'C':[100, 1000], 'tol': [0.001, 0.0001]}\n",
    "log_reg = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "clf = GridSearchCV(log_reg, params, scoring='log_loss', refit='True', n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"best params: \" + str(clf.best_params_))\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "  print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std(), params))\n",
    "  print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO list\n",
    "EDA for data_channel predition\n",
    "\n",
    "calculating recall and precision from confusion matrix. generic function for all models. \n",
    "\n",
    "cross validation for all models.\n",
    "\n",
    "Naive Bayes is throwing import error of sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rubrik\n",
    "### 1\tData Preparation Part 1\t\n",
    "10\t\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "### 2\tData Preparation Part 2\n",
    "5\t\n",
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "###  Modeling and Evaluation 1\t\n",
    "10\tChoose and explain your evaluation metrics that you will use (i.e., accuracy,precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "### \tModeling and Evaluation 2\t\n",
    "10\t\n",
    "Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "\n",
    "### \tModeling and Evaluation 3\t\n",
    "20\t\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "\n",
    "### \tModeling and Evaluation 4\t\n",
    "10\t\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "### \tModeling and Evaluation 5\t\n",
    "10\t\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "\n",
    "### \tModeling and Evaluation 6\t\n",
    "10\t\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "\n",
    "### \tDeployment\t\n",
    "5\t\n",
    "How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? \n",
    "\n",
    "### \tExceptional Work\t\n",
    "10\t\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
