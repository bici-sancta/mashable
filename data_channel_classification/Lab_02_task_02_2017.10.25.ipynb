{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2: predicting data_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter('ignore',DeprecationWarning)\n",
    "import seaborn as sns\n",
    "import time\n",
    "#import hdbscan\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import confusion_matrix as conf\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "data_file = 'OnlineNewsPopularity.csv'\n",
    "\n",
    "file_2_read = data_dir + data_file\n",
    "df = pd.read_csv(file_2_read)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['url',\n",
       " 'timedelta',\n",
       " 'n_tokens_title',\n",
       " 'n_tokens_content',\n",
       " 'n_unique_tokens',\n",
       " 'n_non_stop_words',\n",
       " 'n_non_stop_unique_tokens',\n",
       " 'num_hrefs',\n",
       " 'num_self_hrefs',\n",
       " 'num_imgs',\n",
       " 'num_videos',\n",
       " 'average_token_length',\n",
       " 'num_keywords',\n",
       " 'data_channel_is_lifestyle',\n",
       " 'data_channel_is_entertainment',\n",
       " 'data_channel_is_bus',\n",
       " 'data_channel_is_socmed',\n",
       " 'data_channel_is_tech',\n",
       " 'data_channel_is_world',\n",
       " 'kw_min_min',\n",
       " 'kw_max_min',\n",
       " 'kw_avg_min',\n",
       " 'kw_min_max',\n",
       " 'kw_max_max',\n",
       " 'kw_avg_max',\n",
       " 'kw_min_avg',\n",
       " 'kw_max_avg',\n",
       " 'kw_avg_avg',\n",
       " 'self_reference_min_shares',\n",
       " 'self_reference_max_shares',\n",
       " 'self_reference_avg_sharess',\n",
       " 'weekday_is_monday',\n",
       " 'weekday_is_tuesday',\n",
       " 'weekday_is_wednesday',\n",
       " 'weekday_is_thursday',\n",
       " 'weekday_is_friday',\n",
       " 'weekday_is_saturday',\n",
       " 'weekday_is_sunday',\n",
       " 'is_weekend',\n",
       " 'LDA_00',\n",
       " 'LDA_01',\n",
       " 'LDA_02',\n",
       " 'LDA_03',\n",
       " 'LDA_04',\n",
       " 'global_subjectivity',\n",
       " 'global_sentiment_polarity',\n",
       " 'global_rate_positive_words',\n",
       " 'global_rate_negative_words',\n",
       " 'rate_positive_words',\n",
       " 'rate_negative_words',\n",
       " 'avg_positive_polarity',\n",
       " 'min_positive_polarity',\n",
       " 'max_positive_polarity',\n",
       " 'avg_negative_polarity',\n",
       " 'min_negative_polarity',\n",
       " 'max_negative_polarity',\n",
       " 'title_subjectivity',\n",
       " 'title_sentiment_polarity',\n",
       " 'abs_title_subjectivity',\n",
       " 'abs_title_sentiment_polarity',\n",
       " 'shares']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "col_names = df.columns.values.tolist()\n",
    "\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['n_non_stop_words']\n",
    "del df['n_non_stop_unique_tokens']\n",
    "del df['n_unique_tokens']\n",
    "del df['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>timedelta</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>354.530471</td>\n",
       "      <td>214.163767</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>542.000000</td>\n",
       "      <td>731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_title</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>10.398749</td>\n",
       "      <td>2.114037</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_content</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>546.514731</td>\n",
       "      <td>471.107508</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_hrefs</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>10.883690</td>\n",
       "      <td>11.332017</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>3.293638</td>\n",
       "      <td>3.855141</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>116.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_imgs</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>4.544143</td>\n",
       "      <td>8.309434</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_videos</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1.249874</td>\n",
       "      <td>4.107855</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_token_length</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>4.548239</td>\n",
       "      <td>0.844406</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.478404</td>\n",
       "      <td>4.664082</td>\n",
       "      <td>4.854839</td>\n",
       "      <td>8.041534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_keywords</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>7.223767</td>\n",
       "      <td>1.909130</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.052946</td>\n",
       "      <td>0.223929</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.178009</td>\n",
       "      <td>0.382525</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.157855</td>\n",
       "      <td>0.364610</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.058597</td>\n",
       "      <td>0.234871</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.185299</td>\n",
       "      <td>0.388545</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.212567</td>\n",
       "      <td>0.409129</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_min_min</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>26.106801</td>\n",
       "      <td>69.633215</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>377.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_max_min</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1153.951682</td>\n",
       "      <td>3857.990877</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>298400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_avg_min</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>312.366967</td>\n",
       "      <td>620.783887</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>141.750000</td>\n",
       "      <td>235.500000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>42827.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_min_max</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>13612.354102</td>\n",
       "      <td>57986.029357</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_max_max</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>752324.066694</td>\n",
       "      <td>214502.129573</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_avg_max</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>259281.938083</td>\n",
       "      <td>135102.247285</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>172846.875000</td>\n",
       "      <td>244572.222223</td>\n",
       "      <td>330980.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_min_avg</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1117.146610</td>\n",
       "      <td>1137.456951</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1023.635611</td>\n",
       "      <td>2056.781032</td>\n",
       "      <td>3613.039820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_max_avg</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>5657.211151</td>\n",
       "      <td>6098.871957</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3562.101631</td>\n",
       "      <td>4355.688836</td>\n",
       "      <td>6019.953968</td>\n",
       "      <td>298400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>3135.858639</td>\n",
       "      <td>1318.150397</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2382.448566</td>\n",
       "      <td>2870.074878</td>\n",
       "      <td>3600.229564</td>\n",
       "      <td>43567.659946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>3998.755396</td>\n",
       "      <td>19738.670516</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>639.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>2600.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>10329.212662</td>\n",
       "      <td>41027.576613</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>6401.697580</td>\n",
       "      <td>24211.332231</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>981.187500</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>5200.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.168020</td>\n",
       "      <td>0.373889</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.186409</td>\n",
       "      <td>0.389441</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.187544</td>\n",
       "      <td>0.390353</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.183306</td>\n",
       "      <td>0.386922</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.143805</td>\n",
       "      <td>0.350896</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.061876</td>\n",
       "      <td>0.240933</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.069039</td>\n",
       "      <td>0.253524</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_weekend</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.130915</td>\n",
       "      <td>0.337312</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_00</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.184599</td>\n",
       "      <td>0.262975</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.240958</td>\n",
       "      <td>0.926994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_01</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.141256</td>\n",
       "      <td>0.219707</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.150831</td>\n",
       "      <td>0.925947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_02</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.216321</td>\n",
       "      <td>0.282145</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.334218</td>\n",
       "      <td>0.919999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_03</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.223770</td>\n",
       "      <td>0.295191</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.375763</td>\n",
       "      <td>0.926534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_04</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.234029</td>\n",
       "      <td>0.289183</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.028574</td>\n",
       "      <td>0.040727</td>\n",
       "      <td>0.399986</td>\n",
       "      <td>0.927191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_subjectivity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.443370</td>\n",
       "      <td>0.116685</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.396167</td>\n",
       "      <td>0.453457</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.119309</td>\n",
       "      <td>0.096931</td>\n",
       "      <td>-0.39375</td>\n",
       "      <td>0.057757</td>\n",
       "      <td>0.119117</td>\n",
       "      <td>0.177832</td>\n",
       "      <td>0.727841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.039625</td>\n",
       "      <td>0.017429</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.028384</td>\n",
       "      <td>0.039023</td>\n",
       "      <td>0.050279</td>\n",
       "      <td>0.155488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.015337</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.184932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate_positive_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.682150</td>\n",
       "      <td>0.190206</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate_negative_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.287934</td>\n",
       "      <td>0.156156</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.353825</td>\n",
       "      <td>0.104542</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.306244</td>\n",
       "      <td>0.358755</td>\n",
       "      <td>0.411428</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.095446</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.756728</td>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>-0.259524</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-0.328383</td>\n",
       "      <td>-0.253333</td>\n",
       "      <td>-0.186905</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>-0.521944</td>\n",
       "      <td>0.290290</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>-0.107500</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_subjectivity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.265450</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.188791</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.156064</td>\n",
       "      <td>0.226294</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shares</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>3395.380184</td>\n",
       "      <td>11626.950749</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>946.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 count           mean            std      min  \\\n",
       "timedelta                      39644.0     354.530471     214.163767  8.00000   \n",
       "n_tokens_title                 39644.0      10.398749       2.114037  2.00000   \n",
       "n_tokens_content               39644.0     546.514731     471.107508  0.00000   \n",
       "num_hrefs                      39644.0      10.883690      11.332017  0.00000   \n",
       "num_self_hrefs                 39644.0       3.293638       3.855141  0.00000   \n",
       "num_imgs                       39644.0       4.544143       8.309434  0.00000   \n",
       "num_videos                     39644.0       1.249874       4.107855  0.00000   \n",
       "average_token_length           39644.0       4.548239       0.844406  0.00000   \n",
       "num_keywords                   39644.0       7.223767       1.909130  1.00000   \n",
       "data_channel_is_lifestyle      39644.0       0.052946       0.223929  0.00000   \n",
       "data_channel_is_entertainment  39644.0       0.178009       0.382525  0.00000   \n",
       "data_channel_is_bus            39644.0       0.157855       0.364610  0.00000   \n",
       "data_channel_is_socmed         39644.0       0.058597       0.234871  0.00000   \n",
       "data_channel_is_tech           39644.0       0.185299       0.388545  0.00000   \n",
       "data_channel_is_world          39644.0       0.212567       0.409129  0.00000   \n",
       "kw_min_min                     39644.0      26.106801      69.633215 -1.00000   \n",
       "kw_max_min                     39644.0    1153.951682    3857.990877  0.00000   \n",
       "kw_avg_min                     39644.0     312.366967     620.783887 -1.00000   \n",
       "kw_min_max                     39644.0   13612.354102   57986.029357  0.00000   \n",
       "kw_max_max                     39644.0  752324.066694  214502.129573  0.00000   \n",
       "kw_avg_max                     39644.0  259281.938083  135102.247285  0.00000   \n",
       "kw_min_avg                     39644.0    1117.146610    1137.456951 -1.00000   \n",
       "kw_max_avg                     39644.0    5657.211151    6098.871957  0.00000   \n",
       "kw_avg_avg                     39644.0    3135.858639    1318.150397  0.00000   \n",
       "self_reference_min_shares      39644.0    3998.755396   19738.670516  0.00000   \n",
       "self_reference_max_shares      39644.0   10329.212662   41027.576613  0.00000   \n",
       "self_reference_avg_sharess     39644.0    6401.697580   24211.332231  0.00000   \n",
       "weekday_is_monday              39644.0       0.168020       0.373889  0.00000   \n",
       "weekday_is_tuesday             39644.0       0.186409       0.389441  0.00000   \n",
       "weekday_is_wednesday           39644.0       0.187544       0.390353  0.00000   \n",
       "weekday_is_thursday            39644.0       0.183306       0.386922  0.00000   \n",
       "weekday_is_friday              39644.0       0.143805       0.350896  0.00000   \n",
       "weekday_is_saturday            39644.0       0.061876       0.240933  0.00000   \n",
       "weekday_is_sunday              39644.0       0.069039       0.253524  0.00000   \n",
       "is_weekend                     39644.0       0.130915       0.337312  0.00000   \n",
       "LDA_00                         39644.0       0.184599       0.262975  0.00000   \n",
       "LDA_01                         39644.0       0.141256       0.219707  0.00000   \n",
       "LDA_02                         39644.0       0.216321       0.282145  0.00000   \n",
       "LDA_03                         39644.0       0.223770       0.295191  0.00000   \n",
       "LDA_04                         39644.0       0.234029       0.289183  0.00000   \n",
       "global_subjectivity            39644.0       0.443370       0.116685  0.00000   \n",
       "global_sentiment_polarity      39644.0       0.119309       0.096931 -0.39375   \n",
       "global_rate_positive_words     39644.0       0.039625       0.017429  0.00000   \n",
       "global_rate_negative_words     39644.0       0.016612       0.010828  0.00000   \n",
       "rate_positive_words            39644.0       0.682150       0.190206  0.00000   \n",
       "rate_negative_words            39644.0       0.287934       0.156156  0.00000   \n",
       "avg_positive_polarity          39644.0       0.353825       0.104542  0.00000   \n",
       "min_positive_polarity          39644.0       0.095446       0.071315  0.00000   \n",
       "max_positive_polarity          39644.0       0.756728       0.247786  0.00000   \n",
       "avg_negative_polarity          39644.0      -0.259524       0.127726 -1.00000   \n",
       "min_negative_polarity          39644.0      -0.521944       0.290290 -1.00000   \n",
       "max_negative_polarity          39644.0      -0.107500       0.095373 -1.00000   \n",
       "title_subjectivity             39644.0       0.282353       0.324247  0.00000   \n",
       "title_sentiment_polarity       39644.0       0.071425       0.265450 -1.00000   \n",
       "abs_title_subjectivity         39644.0       0.341843       0.188791  0.00000   \n",
       "abs_title_sentiment_polarity   39644.0       0.156064       0.226294  0.00000   \n",
       "shares                         39644.0    3395.380184   11626.950749  1.00000   \n",
       "\n",
       "                                         25%            50%            75%  \\\n",
       "timedelta                         164.000000     339.000000     542.000000   \n",
       "n_tokens_title                      9.000000      10.000000      12.000000   \n",
       "n_tokens_content                  246.000000     409.000000     716.000000   \n",
       "num_hrefs                           4.000000       8.000000      14.000000   \n",
       "num_self_hrefs                      1.000000       3.000000       4.000000   \n",
       "num_imgs                            1.000000       1.000000       4.000000   \n",
       "num_videos                          0.000000       0.000000       1.000000   \n",
       "average_token_length                4.478404       4.664082       4.854839   \n",
       "num_keywords                        6.000000       7.000000       9.000000   \n",
       "data_channel_is_lifestyle           0.000000       0.000000       0.000000   \n",
       "data_channel_is_entertainment       0.000000       0.000000       0.000000   \n",
       "data_channel_is_bus                 0.000000       0.000000       0.000000   \n",
       "data_channel_is_socmed              0.000000       0.000000       0.000000   \n",
       "data_channel_is_tech                0.000000       0.000000       0.000000   \n",
       "data_channel_is_world               0.000000       0.000000       0.000000   \n",
       "kw_min_min                         -1.000000      -1.000000       4.000000   \n",
       "kw_max_min                        445.000000     660.000000    1000.000000   \n",
       "kw_avg_min                        141.750000     235.500000     357.000000   \n",
       "kw_min_max                          0.000000    1400.000000    7900.000000   \n",
       "kw_max_max                     843300.000000  843300.000000  843300.000000   \n",
       "kw_avg_max                     172846.875000  244572.222223  330980.000000   \n",
       "kw_min_avg                          0.000000    1023.635611    2056.781032   \n",
       "kw_max_avg                       3562.101631    4355.688836    6019.953968   \n",
       "kw_avg_avg                       2382.448566    2870.074878    3600.229564   \n",
       "self_reference_min_shares         639.000000    1200.000000    2600.000000   \n",
       "self_reference_max_shares        1100.000000    2800.000000    8000.000000   \n",
       "self_reference_avg_sharess        981.187500    2200.000000    5200.000000   \n",
       "weekday_is_monday                   0.000000       0.000000       0.000000   \n",
       "weekday_is_tuesday                  0.000000       0.000000       0.000000   \n",
       "weekday_is_wednesday                0.000000       0.000000       0.000000   \n",
       "weekday_is_thursday                 0.000000       0.000000       0.000000   \n",
       "weekday_is_friday                   0.000000       0.000000       0.000000   \n",
       "weekday_is_saturday                 0.000000       0.000000       0.000000   \n",
       "weekday_is_sunday                   0.000000       0.000000       0.000000   \n",
       "is_weekend                          0.000000       0.000000       0.000000   \n",
       "LDA_00                              0.025051       0.033387       0.240958   \n",
       "LDA_01                              0.025012       0.033345       0.150831   \n",
       "LDA_02                              0.028571       0.040004       0.334218   \n",
       "LDA_03                              0.028571       0.040001       0.375763   \n",
       "LDA_04                              0.028574       0.040727       0.399986   \n",
       "global_subjectivity                 0.396167       0.453457       0.508333   \n",
       "global_sentiment_polarity           0.057757       0.119117       0.177832   \n",
       "global_rate_positive_words          0.028384       0.039023       0.050279   \n",
       "global_rate_negative_words          0.009615       0.015337       0.021739   \n",
       "rate_positive_words                 0.600000       0.710526       0.800000   \n",
       "rate_negative_words                 0.185185       0.280000       0.384615   \n",
       "avg_positive_polarity               0.306244       0.358755       0.411428   \n",
       "min_positive_polarity               0.050000       0.100000       0.100000   \n",
       "max_positive_polarity               0.600000       0.800000       1.000000   \n",
       "avg_negative_polarity              -0.328383      -0.253333      -0.186905   \n",
       "min_negative_polarity              -0.700000      -0.500000      -0.300000   \n",
       "max_negative_polarity              -0.125000      -0.100000      -0.050000   \n",
       "title_subjectivity                  0.000000       0.150000       0.500000   \n",
       "title_sentiment_polarity            0.000000       0.000000       0.150000   \n",
       "abs_title_subjectivity              0.166667       0.500000       0.500000   \n",
       "abs_title_sentiment_polarity        0.000000       0.000000       0.250000   \n",
       "shares                            946.000000    1400.000000    2800.000000   \n",
       "\n",
       "                                         max  \n",
       "timedelta                         731.000000  \n",
       "n_tokens_title                     23.000000  \n",
       "n_tokens_content                 8474.000000  \n",
       "num_hrefs                         304.000000  \n",
       "num_self_hrefs                    116.000000  \n",
       "num_imgs                          128.000000  \n",
       "num_videos                         91.000000  \n",
       "average_token_length                8.041534  \n",
       "num_keywords                       10.000000  \n",
       "data_channel_is_lifestyle           1.000000  \n",
       "data_channel_is_entertainment       1.000000  \n",
       "data_channel_is_bus                 1.000000  \n",
       "data_channel_is_socmed              1.000000  \n",
       "data_channel_is_tech                1.000000  \n",
       "data_channel_is_world               1.000000  \n",
       "kw_min_min                        377.000000  \n",
       "kw_max_min                     298400.000000  \n",
       "kw_avg_min                      42827.857143  \n",
       "kw_min_max                     843300.000000  \n",
       "kw_max_max                     843300.000000  \n",
       "kw_avg_max                     843300.000000  \n",
       "kw_min_avg                       3613.039820  \n",
       "kw_max_avg                     298400.000000  \n",
       "kw_avg_avg                      43567.659946  \n",
       "self_reference_min_shares      843300.000000  \n",
       "self_reference_max_shares      843300.000000  \n",
       "self_reference_avg_sharess     843300.000000  \n",
       "weekday_is_monday                   1.000000  \n",
       "weekday_is_tuesday                  1.000000  \n",
       "weekday_is_wednesday                1.000000  \n",
       "weekday_is_thursday                 1.000000  \n",
       "weekday_is_friday                   1.000000  \n",
       "weekday_is_saturday                 1.000000  \n",
       "weekday_is_sunday                   1.000000  \n",
       "is_weekend                          1.000000  \n",
       "LDA_00                              0.926994  \n",
       "LDA_01                              0.925947  \n",
       "LDA_02                              0.919999  \n",
       "LDA_03                              0.926534  \n",
       "LDA_04                              0.927191  \n",
       "global_subjectivity                 1.000000  \n",
       "global_sentiment_polarity           0.727841  \n",
       "global_rate_positive_words          0.155488  \n",
       "global_rate_negative_words          0.184932  \n",
       "rate_positive_words                 1.000000  \n",
       "rate_negative_words                 1.000000  \n",
       "avg_positive_polarity               1.000000  \n",
       "min_positive_polarity               1.000000  \n",
       "max_positive_polarity               1.000000  \n",
       "avg_negative_polarity               0.000000  \n",
       "min_negative_polarity               0.000000  \n",
       "max_negative_polarity               0.000000  \n",
       "title_subjectivity                  1.000000  \n",
       "title_sentiment_polarity            1.000000  \n",
       "abs_title_subjectivity              0.500000  \n",
       "abs_title_sentiment_polarity        1.000000  \n",
       "shares                         843300.000000  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ... creating data_channel categorical variable\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "df['data_channel'] = 'Others'\n",
    "\n",
    "condition = df['data_channel_is_lifestyle'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'Lifestyle'\n",
    "\n",
    "condition = df['data_channel_is_entertainment'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'Entertainment'\n",
    "\n",
    "condition = df['data_channel_is_bus'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'Business'\n",
    "\n",
    "condition = df['data_channel_is_socmed'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'Social Media'\n",
    "\n",
    "condition = df['data_channel_is_tech'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'Technology'\n",
    "\n",
    "condition = df['data_channel_is_world'] == 1\n",
    "df.loc[condition, 'data_channel'] = 'World'\n",
    "\n",
    "del df['data_channel_is_lifestyle']\n",
    "del df['data_channel_is_entertainment']\n",
    "del df['data_channel_is_bus']\n",
    "del df['data_channel_is_socmed']\n",
    "del df['data_channel_is_tech']\n",
    "del df['data_channel_is_world']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "World            8427\n",
       "Technology       7346\n",
       "Entertainment    7057\n",
       "Business         6258\n",
       "Others           6134\n",
       "Social Media     2323\n",
       "Lifestyle        2099\n",
       "Name: data_channel, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.data_channel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ... integer value of categorical values for multinomial NB classification\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "df['data_channel_n'] = 0\n",
    "\n",
    "condition = df['data_channel'] == 'Business'\n",
    "df.loc[condition, 'data_channel_n'] = 1\n",
    "\n",
    "condition = df['data_channel'] == 'Entertainment'\n",
    "df.loc[condition, 'data_channel_n'] = 2\n",
    "\n",
    "condition = df['data_channel'] == 'Lifestyle'\n",
    "df.loc[condition, 'data_channel_n'] = 3\n",
    "\n",
    "condition = df['data_channel'] == 'Others'\n",
    "df.loc[condition, 'data_channel_n'] = 4\n",
    "\n",
    "condition = df['data_channel'] == 'Social Media'\n",
    "df.loc[condition, 'data_channel_n'] = 5\n",
    "\n",
    "condition = df['data_channel'] == 'Technology'\n",
    "df.loc[condition, 'data_channel_n'] = 6\n",
    "\n",
    "condition = df['data_channel'] == 'World'\n",
    "df.loc[condition, 'data_channel_n'] = 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    8427\n",
       "6    7346\n",
       "2    7057\n",
       "1    6258\n",
       "4    6134\n",
       "5    2323\n",
       "3    2099\n",
       "Name: data_channel_n, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.data_channel_n.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  shares is task 1 dependent variable\n",
    "# ...  we are excluding it from this model as per business model this value is not available\n",
    "# ...  during data_channel prediction\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "del df['shares'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  convert the data type to Integer\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "to_int = ['timedelta','n_tokens_title', 'n_tokens_content',\n",
    "    'num_hrefs','num_self_hrefs', 'num_imgs', 'num_videos', 'num_keywords',\n",
    "    'weekday_is_monday',\n",
    "    'weekday_is_tuesday',\n",
    "    'weekday_is_wednesday',\n",
    "    'weekday_is_thursday',\n",
    "    'weekday_is_friday',\n",
    "    'weekday_is_saturday',\n",
    "    'weekday_is_sunday',\n",
    "    'is_weekend',\n",
    "    'data_channel_n']\n",
    "    \n",
    "\n",
    "df[to_int] = df[to_int ].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>data_channel_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timedelta, n_tokens_title, n_tokens_content, num_hrefs, num_self_hrefs, num_imgs, num_videos, average_token_length, num_keywords, kw_min_min, kw_max_min, kw_avg_min, kw_min_max, kw_max_max, kw_avg_max, kw_min_avg, kw_max_avg, kw_avg_avg, self_reference_min_shares, self_reference_max_shares, self_reference_avg_sharess, weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, weekday_is_thursday, weekday_is_friday, weekday_is_saturday, weekday_is_sunday, is_weekend, LDA_00, LDA_01, LDA_02, LDA_03, LDA_04, global_subjectivity, global_sentiment_polarity, global_rate_positive_words, global_rate_negative_words, rate_positive_words, rate_negative_words, avg_positive_polarity, min_positive_polarity, max_positive_polarity, avg_negative_polarity, min_negative_polarity, max_negative_polarity, title_subjectivity, title_sentiment_polarity, abs_title_subjectivity, abs_title_sentiment_polarity, data_channel, data_channel_n]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 52 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> min_value < 0 adjusted :  kw_min_min -1.0\n",
      "--> min_value < 0 adjusted :  kw_avg_min -1.0\n",
      "--> min_value < 0 adjusted :  kw_min_avg -1.0\n",
      "--> min_value < 0 adjusted :  global_sentiment_polarity -0.39375\n",
      "--> min_value < 0 adjusted :  avg_negative_polarity -1.0\n",
      "--> min_value < 0 adjusted :  min_negative_polarity -1.0\n",
      "--> min_value < 0 adjusted :  max_negative_polarity -1.0\n",
      "--> min_value < 0 adjusted :  title_sentiment_polarity -1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>timedelta</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>354.530471</td>\n",
       "      <td>214.163767</td>\n",
       "      <td>8.0</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>542.000000</td>\n",
       "      <td>731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_title</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>10.398749</td>\n",
       "      <td>2.114037</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_content</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>546.514731</td>\n",
       "      <td>471.107508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_hrefs</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>10.883690</td>\n",
       "      <td>11.332017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>3.293638</td>\n",
       "      <td>3.855141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>116.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_imgs</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>4.544143</td>\n",
       "      <td>8.309434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_videos</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1.249874</td>\n",
       "      <td>4.107855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_token_length</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>4.548239</td>\n",
       "      <td>0.844406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.478404</td>\n",
       "      <td>4.664082</td>\n",
       "      <td>4.854839</td>\n",
       "      <td>8.041534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_keywords</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>7.223767</td>\n",
       "      <td>1.909130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_min_min</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>27.106801</td>\n",
       "      <td>69.633215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>378.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_max_min</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1153.951682</td>\n",
       "      <td>3857.990877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>298400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_avg_min</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>313.366967</td>\n",
       "      <td>620.783887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142.750000</td>\n",
       "      <td>236.500000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>42828.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_min_max</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>13612.354102</td>\n",
       "      <td>57986.029357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_max_max</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>752324.066694</td>\n",
       "      <td>214502.129573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_avg_max</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>259281.938083</td>\n",
       "      <td>135102.247285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172846.875000</td>\n",
       "      <td>244572.222223</td>\n",
       "      <td>330980.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_min_avg</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1118.146610</td>\n",
       "      <td>1137.456951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1024.635611</td>\n",
       "      <td>2057.781032</td>\n",
       "      <td>3614.039820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_max_avg</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>5657.211151</td>\n",
       "      <td>6098.871957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3562.101631</td>\n",
       "      <td>4355.688836</td>\n",
       "      <td>6019.953968</td>\n",
       "      <td>298400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>3135.858639</td>\n",
       "      <td>1318.150397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2382.448566</td>\n",
       "      <td>2870.074878</td>\n",
       "      <td>3600.229564</td>\n",
       "      <td>43567.659946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>3998.755396</td>\n",
       "      <td>19738.670516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>639.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>2600.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>10329.212662</td>\n",
       "      <td>41027.576613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>6401.697580</td>\n",
       "      <td>24211.332231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>981.187500</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>5200.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.168020</td>\n",
       "      <td>0.373889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.186409</td>\n",
       "      <td>0.389441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.187544</td>\n",
       "      <td>0.390353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.183306</td>\n",
       "      <td>0.386922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.143805</td>\n",
       "      <td>0.350896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.061876</td>\n",
       "      <td>0.240933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.069039</td>\n",
       "      <td>0.253524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_weekend</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.130915</td>\n",
       "      <td>0.337312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_00</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.184599</td>\n",
       "      <td>0.262975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.240958</td>\n",
       "      <td>0.926994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_01</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.141256</td>\n",
       "      <td>0.219707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.150831</td>\n",
       "      <td>0.925947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_02</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.216321</td>\n",
       "      <td>0.282145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.334218</td>\n",
       "      <td>0.919999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_03</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.223770</td>\n",
       "      <td>0.295191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.375763</td>\n",
       "      <td>0.926534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_04</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.234029</td>\n",
       "      <td>0.289183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028574</td>\n",
       "      <td>0.040727</td>\n",
       "      <td>0.399986</td>\n",
       "      <td>0.927191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_subjectivity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.443370</td>\n",
       "      <td>0.116685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.396167</td>\n",
       "      <td>0.453457</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.513059</td>\n",
       "      <td>0.096931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.451507</td>\n",
       "      <td>0.512867</td>\n",
       "      <td>0.571582</td>\n",
       "      <td>1.121591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.039625</td>\n",
       "      <td>0.017429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028384</td>\n",
       "      <td>0.039023</td>\n",
       "      <td>0.050279</td>\n",
       "      <td>0.155488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.015337</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.184932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate_positive_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.682150</td>\n",
       "      <td>0.190206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate_negative_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.287934</td>\n",
       "      <td>0.156156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.353825</td>\n",
       "      <td>0.104542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306244</td>\n",
       "      <td>0.358755</td>\n",
       "      <td>0.411428</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.095446</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.756728</td>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.740476</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.671617</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.813095</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.478056</td>\n",
       "      <td>0.290290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_subjectivity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1.071425</td>\n",
       "      <td>0.265450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.188791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.156064</td>\n",
       "      <td>0.226294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_n</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>4.184366</td>\n",
       "      <td>2.205607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                count           mean            std  min  \\\n",
       "timedelta                     39644.0     354.530471     214.163767  8.0   \n",
       "n_tokens_title                39644.0      10.398749       2.114037  2.0   \n",
       "n_tokens_content              39644.0     546.514731     471.107508  0.0   \n",
       "num_hrefs                     39644.0      10.883690      11.332017  0.0   \n",
       "num_self_hrefs                39644.0       3.293638       3.855141  0.0   \n",
       "num_imgs                      39644.0       4.544143       8.309434  0.0   \n",
       "num_videos                    39644.0       1.249874       4.107855  0.0   \n",
       "average_token_length          39644.0       4.548239       0.844406  0.0   \n",
       "num_keywords                  39644.0       7.223767       1.909130  1.0   \n",
       "kw_min_min                    39644.0      27.106801      69.633215  0.0   \n",
       "kw_max_min                    39644.0    1153.951682    3857.990877  0.0   \n",
       "kw_avg_min                    39644.0     313.366967     620.783887  0.0   \n",
       "kw_min_max                    39644.0   13612.354102   57986.029357  0.0   \n",
       "kw_max_max                    39644.0  752324.066694  214502.129573  0.0   \n",
       "kw_avg_max                    39644.0  259281.938083  135102.247285  0.0   \n",
       "kw_min_avg                    39644.0    1118.146610    1137.456951  0.0   \n",
       "kw_max_avg                    39644.0    5657.211151    6098.871957  0.0   \n",
       "kw_avg_avg                    39644.0    3135.858639    1318.150397  0.0   \n",
       "self_reference_min_shares     39644.0    3998.755396   19738.670516  0.0   \n",
       "self_reference_max_shares     39644.0   10329.212662   41027.576613  0.0   \n",
       "self_reference_avg_sharess    39644.0    6401.697580   24211.332231  0.0   \n",
       "weekday_is_monday             39644.0       0.168020       0.373889  0.0   \n",
       "weekday_is_tuesday            39644.0       0.186409       0.389441  0.0   \n",
       "weekday_is_wednesday          39644.0       0.187544       0.390353  0.0   \n",
       "weekday_is_thursday           39644.0       0.183306       0.386922  0.0   \n",
       "weekday_is_friday             39644.0       0.143805       0.350896  0.0   \n",
       "weekday_is_saturday           39644.0       0.061876       0.240933  0.0   \n",
       "weekday_is_sunday             39644.0       0.069039       0.253524  0.0   \n",
       "is_weekend                    39644.0       0.130915       0.337312  0.0   \n",
       "LDA_00                        39644.0       0.184599       0.262975  0.0   \n",
       "LDA_01                        39644.0       0.141256       0.219707  0.0   \n",
       "LDA_02                        39644.0       0.216321       0.282145  0.0   \n",
       "LDA_03                        39644.0       0.223770       0.295191  0.0   \n",
       "LDA_04                        39644.0       0.234029       0.289183  0.0   \n",
       "global_subjectivity           39644.0       0.443370       0.116685  0.0   \n",
       "global_sentiment_polarity     39644.0       0.513059       0.096931  0.0   \n",
       "global_rate_positive_words    39644.0       0.039625       0.017429  0.0   \n",
       "global_rate_negative_words    39644.0       0.016612       0.010828  0.0   \n",
       "rate_positive_words           39644.0       0.682150       0.190206  0.0   \n",
       "rate_negative_words           39644.0       0.287934       0.156156  0.0   \n",
       "avg_positive_polarity         39644.0       0.353825       0.104542  0.0   \n",
       "min_positive_polarity         39644.0       0.095446       0.071315  0.0   \n",
       "max_positive_polarity         39644.0       0.756728       0.247786  0.0   \n",
       "avg_negative_polarity         39644.0       0.740476       0.127726  0.0   \n",
       "min_negative_polarity         39644.0       0.478056       0.290290  0.0   \n",
       "max_negative_polarity         39644.0       0.892500       0.095373  0.0   \n",
       "title_subjectivity            39644.0       0.282353       0.324247  0.0   \n",
       "title_sentiment_polarity      39644.0       1.071425       0.265450  0.0   \n",
       "abs_title_subjectivity        39644.0       0.341843       0.188791  0.0   \n",
       "abs_title_sentiment_polarity  39644.0       0.156064       0.226294  0.0   \n",
       "data_channel_n                39644.0       4.184366       2.205607  1.0   \n",
       "\n",
       "                                        25%            50%            75%  \\\n",
       "timedelta                        164.000000     339.000000     542.000000   \n",
       "n_tokens_title                     9.000000      10.000000      12.000000   \n",
       "n_tokens_content                 246.000000     409.000000     716.000000   \n",
       "num_hrefs                          4.000000       8.000000      14.000000   \n",
       "num_self_hrefs                     1.000000       3.000000       4.000000   \n",
       "num_imgs                           1.000000       1.000000       4.000000   \n",
       "num_videos                         0.000000       0.000000       1.000000   \n",
       "average_token_length               4.478404       4.664082       4.854839   \n",
       "num_keywords                       6.000000       7.000000       9.000000   \n",
       "kw_min_min                         0.000000       0.000000       5.000000   \n",
       "kw_max_min                       445.000000     660.000000    1000.000000   \n",
       "kw_avg_min                       142.750000     236.500000     358.000000   \n",
       "kw_min_max                         0.000000    1400.000000    7900.000000   \n",
       "kw_max_max                    843300.000000  843300.000000  843300.000000   \n",
       "kw_avg_max                    172846.875000  244572.222223  330980.000000   \n",
       "kw_min_avg                         1.000000    1024.635611    2057.781032   \n",
       "kw_max_avg                      3562.101631    4355.688836    6019.953968   \n",
       "kw_avg_avg                      2382.448566    2870.074878    3600.229564   \n",
       "self_reference_min_shares        639.000000    1200.000000    2600.000000   \n",
       "self_reference_max_shares       1100.000000    2800.000000    8000.000000   \n",
       "self_reference_avg_sharess       981.187500    2200.000000    5200.000000   \n",
       "weekday_is_monday                  0.000000       0.000000       0.000000   \n",
       "weekday_is_tuesday                 0.000000       0.000000       0.000000   \n",
       "weekday_is_wednesday               0.000000       0.000000       0.000000   \n",
       "weekday_is_thursday                0.000000       0.000000       0.000000   \n",
       "weekday_is_friday                  0.000000       0.000000       0.000000   \n",
       "weekday_is_saturday                0.000000       0.000000       0.000000   \n",
       "weekday_is_sunday                  0.000000       0.000000       0.000000   \n",
       "is_weekend                         0.000000       0.000000       0.000000   \n",
       "LDA_00                             0.025051       0.033387       0.240958   \n",
       "LDA_01                             0.025012       0.033345       0.150831   \n",
       "LDA_02                             0.028571       0.040004       0.334218   \n",
       "LDA_03                             0.028571       0.040001       0.375763   \n",
       "LDA_04                             0.028574       0.040727       0.399986   \n",
       "global_subjectivity                0.396167       0.453457       0.508333   \n",
       "global_sentiment_polarity          0.451507       0.512867       0.571582   \n",
       "global_rate_positive_words         0.028384       0.039023       0.050279   \n",
       "global_rate_negative_words         0.009615       0.015337       0.021739   \n",
       "rate_positive_words                0.600000       0.710526       0.800000   \n",
       "rate_negative_words                0.185185       0.280000       0.384615   \n",
       "avg_positive_polarity              0.306244       0.358755       0.411428   \n",
       "min_positive_polarity              0.050000       0.100000       0.100000   \n",
       "max_positive_polarity              0.600000       0.800000       1.000000   \n",
       "avg_negative_polarity              0.671617       0.746667       0.813095   \n",
       "min_negative_polarity              0.300000       0.500000       0.700000   \n",
       "max_negative_polarity              0.875000       0.900000       0.950000   \n",
       "title_subjectivity                 0.000000       0.150000       0.500000   \n",
       "title_sentiment_polarity           1.000000       1.000000       1.150000   \n",
       "abs_title_subjectivity             0.166667       0.500000       0.500000   \n",
       "abs_title_sentiment_polarity       0.000000       0.000000       0.250000   \n",
       "data_channel_n                     2.000000       4.000000       6.000000   \n",
       "\n",
       "                                        max  \n",
       "timedelta                        731.000000  \n",
       "n_tokens_title                    23.000000  \n",
       "n_tokens_content                8474.000000  \n",
       "num_hrefs                        304.000000  \n",
       "num_self_hrefs                   116.000000  \n",
       "num_imgs                         128.000000  \n",
       "num_videos                        91.000000  \n",
       "average_token_length               8.041534  \n",
       "num_keywords                      10.000000  \n",
       "kw_min_min                       378.000000  \n",
       "kw_max_min                    298400.000000  \n",
       "kw_avg_min                     42828.857143  \n",
       "kw_min_max                    843300.000000  \n",
       "kw_max_max                    843300.000000  \n",
       "kw_avg_max                    843300.000000  \n",
       "kw_min_avg                      3614.039820  \n",
       "kw_max_avg                    298400.000000  \n",
       "kw_avg_avg                     43567.659946  \n",
       "self_reference_min_shares     843300.000000  \n",
       "self_reference_max_shares     843300.000000  \n",
       "self_reference_avg_sharess    843300.000000  \n",
       "weekday_is_monday                  1.000000  \n",
       "weekday_is_tuesday                 1.000000  \n",
       "weekday_is_wednesday               1.000000  \n",
       "weekday_is_thursday                1.000000  \n",
       "weekday_is_friday                  1.000000  \n",
       "weekday_is_saturday                1.000000  \n",
       "weekday_is_sunday                  1.000000  \n",
       "is_weekend                         1.000000  \n",
       "LDA_00                             0.926994  \n",
       "LDA_01                             0.925947  \n",
       "LDA_02                             0.919999  \n",
       "LDA_03                             0.926534  \n",
       "LDA_04                             0.927191  \n",
       "global_subjectivity                1.000000  \n",
       "global_sentiment_polarity          1.121591  \n",
       "global_rate_positive_words         0.155488  \n",
       "global_rate_negative_words         0.184932  \n",
       "rate_positive_words                1.000000  \n",
       "rate_negative_words                1.000000  \n",
       "avg_positive_polarity              1.000000  \n",
       "min_positive_polarity              1.000000  \n",
       "max_positive_polarity              1.000000  \n",
       "avg_negative_polarity              1.000000  \n",
       "min_negative_polarity              1.000000  \n",
       "max_negative_polarity              1.000000  \n",
       "title_subjectivity                 1.000000  \n",
       "title_sentiment_polarity           2.000000  \n",
       "abs_title_subjectivity             0.500000  \n",
       "abs_title_sentiment_polarity       1.000000  \n",
       "data_channel_n                     7.000000  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  for all columns with negative values, add +1 to all values in the column\n",
    "# ...  - the only columns with negative values are polarity / sentiment measures\n",
    "# ...  - adding a constant to all values does not modify distributions\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "df_numeric = df.select_dtypes(['number'])\n",
    "numeric_col_names = df_numeric.columns.values.tolist()\n",
    "\n",
    "# ... store min value for each column\n",
    "\n",
    "df_mins = df.min()\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  loop on each column, test for min < 0, add constant as applicable\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "for column in numeric_col_names :\n",
    "    if df_mins[column] < 0 :\n",
    "        df[column] = df[column] - df_mins[column]\n",
    "        print('--> min_value < 0 adjusted : ', column, df_mins[column])\n",
    "        \n",
    "        \n",
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_content 2.94542193879 ln_n_tokens_content\n",
      "num_hrefs 4.0134948282 ln_num_hrefs\n",
      "num_self_hrefs 5.17275110576 ln_num_self_hrefs\n",
      "num_imgs 3.94659584465 ln_num_imgs\n",
      "num_videos 7.0195327863 ln_num_videos\n",
      "kw_min_min 2.37494728018 ln_kw_min_min\n",
      "kw_max_min 35.3284337312 ln_kw_max_min\n",
      "kw_avg_min 31.3061081027 ln_kw_avg_min\n",
      "kw_min_max 10.3863716348 ln_kw_min_max\n",
      "kw_max_avg 16.4116695554 ln_kw_max_avg\n",
      "kw_avg_avg 5.76017729162 ln_kw_avg_avg\n",
      "self_reference_min_shares 26.2643641603 ln_self_reference_min_shares\n",
      "self_reference_max_shares 13.8708490494 ln_self_reference_max_shares\n",
      "self_reference_avg_sharess 17.9140933777 ln_self_reference_avg_sharess\n",
      "weekday_is_monday 1.77590824423 ln_weekday_is_monday\n",
      "weekday_is_tuesday 1.61054706191 ln_weekday_is_tuesday\n",
      "weekday_is_wednesday 1.60097097689 ln_weekday_is_wednesday\n",
      "weekday_is_thursday 1.6370700483 ln_weekday_is_thursday\n",
      "weekday_is_friday 2.03030483518 ln_weekday_is_friday\n",
      "weekday_is_saturday 3.63708575997 ln_weekday_is_saturday\n",
      "weekday_is_sunday 3.3999273763 ln_weekday_is_sunday\n",
      "is_weekend 2.18850033431 ln_is_weekend\n",
      "LDA_00 1.5674632332 ln_LDA_00\n",
      "LDA_01 2.08672182342 ln_LDA_01\n",
      "LDA_02 1.31169490203 ln_LDA_02\n",
      "LDA_03 1.23871598638 ln_LDA_03\n",
      "LDA_04 1.17312947598 ln_LDA_04\n",
      "global_rate_negative_words 1.49191730919 ln_global_rate_negative_words\n",
      "min_positive_polarity 3.04046773746 ln_min_positive_polarity\n",
      "abs_title_sentiment_polarity 1.70419343991 ln_abs_title_sentiment_polarity\n",
      "['n_tokens_content', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max', 'kw_max_avg', 'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_rate_negative_words', 'min_positive_polarity', 'abs_title_sentiment_polarity']\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Number of current columns in dataset : 69\n"
     ]
    }
   ],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  ln() transform right skewed distribution variables (skewness > 1)\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "df_numeric = df.select_dtypes(['number'])\n",
    "\n",
    "numeric_col_names = df_numeric.columns.values.tolist()\n",
    "\n",
    "# ... store min value for each column\n",
    "\n",
    "df_mins = df.min()\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  loop on each column, test for skewness, create new column if conditions met\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "columns_to_drop = []\n",
    "\n",
    "for column in numeric_col_names:\n",
    "    sk = df[column].skew()\n",
    "    \n",
    "    if(sk > 1):\n",
    "        new_col_name = 'ln_' + column\n",
    "        print (column, sk, new_col_name)\n",
    "        \n",
    "        if df_mins[column] > 0:\n",
    "            df[new_col_name] = np.log(df[column])\n",
    "            columns_to_drop.append(column)\n",
    "            \n",
    "        elif df_mins[column] == 0:\n",
    "            df_tmp = df[column] + 1\n",
    "            df[new_col_name] = np.log(df_tmp)\n",
    "            columns_to_drop.append(column)\n",
    "            \n",
    "        else:\n",
    "            print('--> Ln() transform not completed -- skew > 1, but min value < 0 :', column, '!!')\n",
    "            \n",
    "            \n",
    "# ... delete tmp data\n",
    "\n",
    "del df_tmp\n",
    "del df_mins\n",
    "del df_numeric\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  based on inspection, a few of these are just not valid ranges in ln() space\n",
    "# ...  -- just delete these few back out of the data set\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "print (columns_to_drop)\n",
    "\n",
    "del df['ln_LDA_00']\n",
    "del df['ln_LDA_01']\n",
    "del df['ln_LDA_02']\n",
    "del df['ln_LDA_03']\n",
    "del df['ln_LDA_04']\n",
    "columns_to_drop.remove('LDA_00')\n",
    "columns_to_drop.remove('LDA_01')\n",
    "columns_to_drop.remove('LDA_02')\n",
    "columns_to_drop.remove('LDA_03')\n",
    "columns_to_drop.remove('LDA_04')\n",
    "\n",
    "# ...  these are binary indicators ... so no need to ln-transform\n",
    "\n",
    "del df['ln_weekday_is_monday']\n",
    "del df['ln_weekday_is_tuesday']\n",
    "del df['ln_weekday_is_wednesday']\n",
    "del df['ln_weekday_is_thursday']\n",
    "del df['ln_weekday_is_friday']\n",
    "del df['ln_weekday_is_saturday']\n",
    "del df['ln_weekday_is_sunday']\n",
    "del df['ln_is_weekend']\n",
    "columns_to_drop.remove('is_weekend')\n",
    "columns_to_drop.remove('weekday_is_monday')\n",
    "columns_to_drop.remove('weekday_is_tuesday')\n",
    "columns_to_drop.remove('weekday_is_wednesday')\n",
    "columns_to_drop.remove('weekday_is_thursday')\n",
    "columns_to_drop.remove('weekday_is_friday')\n",
    "columns_to_drop.remove('weekday_is_saturday')\n",
    "columns_to_drop.remove('weekday_is_sunday')\n",
    "\n",
    "#columns_to_drop.remove('data_channel')\n",
    "\n",
    "print ('\\n-----------------------------------\\n')\n",
    "print ('Number of current columns in dataset :', len(df.columns))\n",
    "\n",
    "df.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #TODO: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timedelta', 'n_tokens_title', 'average_token_length', 'num_keywords',\n",
       "       'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'weekday_is_monday',\n",
       "       'weekday_is_tuesday', 'weekday_is_wednesday', 'weekday_is_thursday',\n",
       "       'weekday_is_friday', 'weekday_is_saturday', 'weekday_is_sunday',\n",
       "       'is_weekend', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04',\n",
       "       'global_subjectivity', 'global_sentiment_polarity',\n",
       "       'global_rate_positive_words', 'rate_positive_words',\n",
       "       'rate_negative_words', 'avg_positive_polarity', 'max_positive_polarity',\n",
       "       'avg_negative_polarity', 'min_negative_polarity',\n",
       "       'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity', 'data_channel',\n",
       "       'data_channel_n', 'ln_n_tokens_content', 'ln_num_hrefs',\n",
       "       'ln_num_self_hrefs', 'ln_num_imgs', 'ln_num_videos', 'ln_kw_min_min',\n",
       "       'ln_kw_max_min', 'ln_kw_avg_min', 'ln_kw_min_max', 'ln_kw_max_avg',\n",
       "       'ln_kw_avg_avg', 'ln_self_reference_min_shares',\n",
       "       'ln_self_reference_max_shares', 'ln_self_reference_avg_sharess',\n",
       "       'ln_global_rate_negative_words', 'ln_min_positive_polarity',\n",
       "       'ln_abs_title_sentiment_polarity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>timedelta</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>354.530471</td>\n",
       "      <td>214.163767</td>\n",
       "      <td>8.0</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>542.000000</td>\n",
       "      <td>731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_title</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>10.398749</td>\n",
       "      <td>2.114037</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_token_length</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>4.548239</td>\n",
       "      <td>0.844406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.478404</td>\n",
       "      <td>4.664082</td>\n",
       "      <td>4.854839</td>\n",
       "      <td>8.041534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_keywords</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>7.223767</td>\n",
       "      <td>1.909130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_max_max</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>752324.066694</td>\n",
       "      <td>214502.129573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_avg_max</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>259281.938083</td>\n",
       "      <td>135102.247285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172846.875000</td>\n",
       "      <td>244572.222223</td>\n",
       "      <td>330980.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_min_avg</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1118.146610</td>\n",
       "      <td>1137.456951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1024.635611</td>\n",
       "      <td>2057.781032</td>\n",
       "      <td>3614.039820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.168020</td>\n",
       "      <td>0.373889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.186409</td>\n",
       "      <td>0.389441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.187544</td>\n",
       "      <td>0.390353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.183306</td>\n",
       "      <td>0.386922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.143805</td>\n",
       "      <td>0.350896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.061876</td>\n",
       "      <td>0.240933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.069039</td>\n",
       "      <td>0.253524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_weekend</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.130915</td>\n",
       "      <td>0.337312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_00</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.184599</td>\n",
       "      <td>0.262975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.240958</td>\n",
       "      <td>0.926994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_01</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.141256</td>\n",
       "      <td>0.219707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.150831</td>\n",
       "      <td>0.925947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_02</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.216321</td>\n",
       "      <td>0.282145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.334218</td>\n",
       "      <td>0.919999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_03</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.223770</td>\n",
       "      <td>0.295191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.375763</td>\n",
       "      <td>0.926534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_04</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.234029</td>\n",
       "      <td>0.289183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028574</td>\n",
       "      <td>0.040727</td>\n",
       "      <td>0.399986</td>\n",
       "      <td>0.927191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_subjectivity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.443370</td>\n",
       "      <td>0.116685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.396167</td>\n",
       "      <td>0.453457</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.513059</td>\n",
       "      <td>0.096931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.451507</td>\n",
       "      <td>0.512867</td>\n",
       "      <td>0.571582</td>\n",
       "      <td>1.121591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.039625</td>\n",
       "      <td>0.017429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028384</td>\n",
       "      <td>0.039023</td>\n",
       "      <td>0.050279</td>\n",
       "      <td>0.155488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate_positive_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.682150</td>\n",
       "      <td>0.190206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate_negative_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.287934</td>\n",
       "      <td>0.156156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.353825</td>\n",
       "      <td>0.104542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306244</td>\n",
       "      <td>0.358755</td>\n",
       "      <td>0.411428</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.756728</td>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.740476</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.671617</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.813095</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.478056</td>\n",
       "      <td>0.290290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_subjectivity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1.071425</td>\n",
       "      <td>0.265450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.188791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_n</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>4.184366</td>\n",
       "      <td>2.205607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_n_tokens_content</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>5.889971</td>\n",
       "      <td>1.255442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.509388</td>\n",
       "      <td>6.016157</td>\n",
       "      <td>6.575076</td>\n",
       "      <td>9.044876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_num_hrefs</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>2.156564</td>\n",
       "      <td>0.809445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>5.720312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_num_self_hrefs</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1.208878</td>\n",
       "      <td>0.692698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>4.762174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_num_imgs</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1.116427</td>\n",
       "      <td>0.973755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>4.859812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_num_videos</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.400420</td>\n",
       "      <td>0.680486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>4.521789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_kw_min_min</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>1.174410</td>\n",
       "      <td>1.733030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>5.937536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_kw_max_min</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>6.393888</td>\n",
       "      <td>1.311168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.100319</td>\n",
       "      <td>6.493754</td>\n",
       "      <td>6.908755</td>\n",
       "      <td>12.606193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_kw_avg_min</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>5.302209</td>\n",
       "      <td>1.132463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.968076</td>\n",
       "      <td>5.470168</td>\n",
       "      <td>5.883322</td>\n",
       "      <td>10.664991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_kw_min_max</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>5.045209</td>\n",
       "      <td>4.521016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.244942</td>\n",
       "      <td>8.974745</td>\n",
       "      <td>13.645079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_kw_max_avg</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>8.482750</td>\n",
       "      <td>0.582051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.178387</td>\n",
       "      <td>8.379468</td>\n",
       "      <td>8.703001</td>\n",
       "      <td>12.606193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_kw_avg_avg</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>7.976327</td>\n",
       "      <td>0.489467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.776304</td>\n",
       "      <td>7.962442</td>\n",
       "      <td>8.189031</td>\n",
       "      <td>10.682093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_self_reference_min_shares</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>6.195185</td>\n",
       "      <td>3.076913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.461468</td>\n",
       "      <td>7.090910</td>\n",
       "      <td>7.863651</td>\n",
       "      <td>13.645079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_self_reference_max_shares</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>6.917477</td>\n",
       "      <td>3.432430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>7.937732</td>\n",
       "      <td>8.987322</td>\n",
       "      <td>13.645079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_self_reference_avg_sharess</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>6.667697</td>\n",
       "      <td>3.280186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.889782</td>\n",
       "      <td>7.696667</td>\n",
       "      <td>8.556606</td>\n",
       "      <td>13.645079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_global_rate_negative_words</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.016419</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>0.015221</td>\n",
       "      <td>0.021506</td>\n",
       "      <td>0.169685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_min_positive_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.089255</td>\n",
       "      <td>0.060260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048790</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_abs_title_sentiment_polarity</th>\n",
       "      <td>39644.0</td>\n",
       "      <td>0.128709</td>\n",
       "      <td>0.173844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223144</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   count           mean            std  min  \\\n",
       "timedelta                        39644.0     354.530471     214.163767  8.0   \n",
       "n_tokens_title                   39644.0      10.398749       2.114037  2.0   \n",
       "average_token_length             39644.0       4.548239       0.844406  0.0   \n",
       "num_keywords                     39644.0       7.223767       1.909130  1.0   \n",
       "kw_max_max                       39644.0  752324.066694  214502.129573  0.0   \n",
       "kw_avg_max                       39644.0  259281.938083  135102.247285  0.0   \n",
       "kw_min_avg                       39644.0    1118.146610    1137.456951  0.0   \n",
       "weekday_is_monday                39644.0       0.168020       0.373889  0.0   \n",
       "weekday_is_tuesday               39644.0       0.186409       0.389441  0.0   \n",
       "weekday_is_wednesday             39644.0       0.187544       0.390353  0.0   \n",
       "weekday_is_thursday              39644.0       0.183306       0.386922  0.0   \n",
       "weekday_is_friday                39644.0       0.143805       0.350896  0.0   \n",
       "weekday_is_saturday              39644.0       0.061876       0.240933  0.0   \n",
       "weekday_is_sunday                39644.0       0.069039       0.253524  0.0   \n",
       "is_weekend                       39644.0       0.130915       0.337312  0.0   \n",
       "LDA_00                           39644.0       0.184599       0.262975  0.0   \n",
       "LDA_01                           39644.0       0.141256       0.219707  0.0   \n",
       "LDA_02                           39644.0       0.216321       0.282145  0.0   \n",
       "LDA_03                           39644.0       0.223770       0.295191  0.0   \n",
       "LDA_04                           39644.0       0.234029       0.289183  0.0   \n",
       "global_subjectivity              39644.0       0.443370       0.116685  0.0   \n",
       "global_sentiment_polarity        39644.0       0.513059       0.096931  0.0   \n",
       "global_rate_positive_words       39644.0       0.039625       0.017429  0.0   \n",
       "rate_positive_words              39644.0       0.682150       0.190206  0.0   \n",
       "rate_negative_words              39644.0       0.287934       0.156156  0.0   \n",
       "avg_positive_polarity            39644.0       0.353825       0.104542  0.0   \n",
       "max_positive_polarity            39644.0       0.756728       0.247786  0.0   \n",
       "avg_negative_polarity            39644.0       0.740476       0.127726  0.0   \n",
       "min_negative_polarity            39644.0       0.478056       0.290290  0.0   \n",
       "max_negative_polarity            39644.0       0.892500       0.095373  0.0   \n",
       "title_subjectivity               39644.0       0.282353       0.324247  0.0   \n",
       "title_sentiment_polarity         39644.0       1.071425       0.265450  0.0   \n",
       "abs_title_subjectivity           39644.0       0.341843       0.188791  0.0   \n",
       "data_channel_n                   39644.0       4.184366       2.205607  1.0   \n",
       "ln_n_tokens_content              39644.0       5.889971       1.255442  0.0   \n",
       "ln_num_hrefs                     39644.0       2.156564       0.809445  0.0   \n",
       "ln_num_self_hrefs                39644.0       1.208878       0.692698  0.0   \n",
       "ln_num_imgs                      39644.0       1.116427       0.973755  0.0   \n",
       "ln_num_videos                    39644.0       0.400420       0.680486  0.0   \n",
       "ln_kw_min_min                    39644.0       1.174410       1.733030  0.0   \n",
       "ln_kw_max_min                    39644.0       6.393888       1.311168  0.0   \n",
       "ln_kw_avg_min                    39644.0       5.302209       1.132463  0.0   \n",
       "ln_kw_min_max                    39644.0       5.045209       4.521016  0.0   \n",
       "ln_kw_max_avg                    39644.0       8.482750       0.582051  0.0   \n",
       "ln_kw_avg_avg                    39644.0       7.976327       0.489467  0.0   \n",
       "ln_self_reference_min_shares     39644.0       6.195185       3.076913  0.0   \n",
       "ln_self_reference_max_shares     39644.0       6.917477       3.432430  0.0   \n",
       "ln_self_reference_avg_sharess    39644.0       6.667697       3.280186  0.0   \n",
       "ln_global_rate_negative_words    39644.0       0.016419       0.010571  0.0   \n",
       "ln_min_positive_polarity         39644.0       0.089255       0.060260  0.0   \n",
       "ln_abs_title_sentiment_polarity  39644.0       0.128709       0.173844  0.0   \n",
       "\n",
       "                                           25%            50%            75%  \\\n",
       "timedelta                           164.000000     339.000000     542.000000   \n",
       "n_tokens_title                        9.000000      10.000000      12.000000   \n",
       "average_token_length                  4.478404       4.664082       4.854839   \n",
       "num_keywords                          6.000000       7.000000       9.000000   \n",
       "kw_max_max                       843300.000000  843300.000000  843300.000000   \n",
       "kw_avg_max                       172846.875000  244572.222223  330980.000000   \n",
       "kw_min_avg                            1.000000    1024.635611    2057.781032   \n",
       "weekday_is_monday                     0.000000       0.000000       0.000000   \n",
       "weekday_is_tuesday                    0.000000       0.000000       0.000000   \n",
       "weekday_is_wednesday                  0.000000       0.000000       0.000000   \n",
       "weekday_is_thursday                   0.000000       0.000000       0.000000   \n",
       "weekday_is_friday                     0.000000       0.000000       0.000000   \n",
       "weekday_is_saturday                   0.000000       0.000000       0.000000   \n",
       "weekday_is_sunday                     0.000000       0.000000       0.000000   \n",
       "is_weekend                            0.000000       0.000000       0.000000   \n",
       "LDA_00                                0.025051       0.033387       0.240958   \n",
       "LDA_01                                0.025012       0.033345       0.150831   \n",
       "LDA_02                                0.028571       0.040004       0.334218   \n",
       "LDA_03                                0.028571       0.040001       0.375763   \n",
       "LDA_04                                0.028574       0.040727       0.399986   \n",
       "global_subjectivity                   0.396167       0.453457       0.508333   \n",
       "global_sentiment_polarity             0.451507       0.512867       0.571582   \n",
       "global_rate_positive_words            0.028384       0.039023       0.050279   \n",
       "rate_positive_words                   0.600000       0.710526       0.800000   \n",
       "rate_negative_words                   0.185185       0.280000       0.384615   \n",
       "avg_positive_polarity                 0.306244       0.358755       0.411428   \n",
       "max_positive_polarity                 0.600000       0.800000       1.000000   \n",
       "avg_negative_polarity                 0.671617       0.746667       0.813095   \n",
       "min_negative_polarity                 0.300000       0.500000       0.700000   \n",
       "max_negative_polarity                 0.875000       0.900000       0.950000   \n",
       "title_subjectivity                    0.000000       0.150000       0.500000   \n",
       "title_sentiment_polarity              1.000000       1.000000       1.150000   \n",
       "abs_title_subjectivity                0.166667       0.500000       0.500000   \n",
       "data_channel_n                        2.000000       4.000000       6.000000   \n",
       "ln_n_tokens_content                   5.509388       6.016157       6.575076   \n",
       "ln_num_hrefs                          1.609438       2.197225       2.708050   \n",
       "ln_num_self_hrefs                     0.693147       1.386294       1.609438   \n",
       "ln_num_imgs                           0.693147       0.693147       1.609438   \n",
       "ln_num_videos                         0.000000       0.000000       0.693147   \n",
       "ln_kw_min_min                         0.000000       0.000000       1.791759   \n",
       "ln_kw_max_min                         6.100319       6.493754       6.908755   \n",
       "ln_kw_avg_min                         4.968076       5.470168       5.883322   \n",
       "ln_kw_min_max                         0.000000       7.244942       8.974745   \n",
       "ln_kw_max_avg                         8.178387       8.379468       8.703001   \n",
       "ln_kw_avg_avg                         7.776304       7.962442       8.189031   \n",
       "ln_self_reference_min_shares          6.461468       7.090910       7.863651   \n",
       "ln_self_reference_max_shares          7.003974       7.937732       8.987322   \n",
       "ln_self_reference_avg_sharess         6.889782       7.696667       8.556606   \n",
       "ln_global_rate_negative_words         0.009569       0.015221       0.021506   \n",
       "ln_min_positive_polarity              0.048790       0.095310       0.095310   \n",
       "ln_abs_title_sentiment_polarity       0.000000       0.000000       0.223144   \n",
       "\n",
       "                                           max  \n",
       "timedelta                           731.000000  \n",
       "n_tokens_title                       23.000000  \n",
       "average_token_length                  8.041534  \n",
       "num_keywords                         10.000000  \n",
       "kw_max_max                       843300.000000  \n",
       "kw_avg_max                       843300.000000  \n",
       "kw_min_avg                         3614.039820  \n",
       "weekday_is_monday                     1.000000  \n",
       "weekday_is_tuesday                    1.000000  \n",
       "weekday_is_wednesday                  1.000000  \n",
       "weekday_is_thursday                   1.000000  \n",
       "weekday_is_friday                     1.000000  \n",
       "weekday_is_saturday                   1.000000  \n",
       "weekday_is_sunday                     1.000000  \n",
       "is_weekend                            1.000000  \n",
       "LDA_00                                0.926994  \n",
       "LDA_01                                0.925947  \n",
       "LDA_02                                0.919999  \n",
       "LDA_03                                0.926534  \n",
       "LDA_04                                0.927191  \n",
       "global_subjectivity                   1.000000  \n",
       "global_sentiment_polarity             1.121591  \n",
       "global_rate_positive_words            0.155488  \n",
       "rate_positive_words                   1.000000  \n",
       "rate_negative_words                   1.000000  \n",
       "avg_positive_polarity                 1.000000  \n",
       "max_positive_polarity                 1.000000  \n",
       "avg_negative_polarity                 1.000000  \n",
       "min_negative_polarity                 1.000000  \n",
       "max_negative_polarity                 1.000000  \n",
       "title_subjectivity                    1.000000  \n",
       "title_sentiment_polarity              2.000000  \n",
       "abs_title_subjectivity                0.500000  \n",
       "data_channel_n                        7.000000  \n",
       "ln_n_tokens_content                   9.044876  \n",
       "ln_num_hrefs                          5.720312  \n",
       "ln_num_self_hrefs                     4.762174  \n",
       "ln_num_imgs                           4.859812  \n",
       "ln_num_videos                         4.521789  \n",
       "ln_kw_min_min                         5.937536  \n",
       "ln_kw_max_min                        12.606193  \n",
       "ln_kw_avg_min                        10.664991  \n",
       "ln_kw_min_max                        13.645079  \n",
       "ln_kw_max_avg                        12.606193  \n",
       "ln_kw_avg_avg                        10.682093  \n",
       "ln_self_reference_min_shares         13.645079  \n",
       "ln_self_reference_max_shares         13.645079  \n",
       "ln_self_reference_avg_sharess        13.645079  \n",
       "ln_global_rate_negative_words         0.169685  \n",
       "ln_min_positive_polarity              0.693147  \n",
       "ln_abs_title_sentiment_polarity       0.693147  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "World            8427\n",
       "Technology       7346\n",
       "Entertainment    7057\n",
       "Business         6258\n",
       "Others           6134\n",
       "Social Media     2323\n",
       "Lifestyle        2099\n",
       "Name: data_channel, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "df.columns\n",
    "\n",
    "df.describe().T\n",
    "\n",
    "df.data_channel.value_counts()\n",
    "\n",
    "data_dir = '../data/'\n",
    "data_file = 'mashable_clean_dataset_for_lab_02_task_02.csv'\n",
    "\n",
    "file_2_write = data_dir + data_file\n",
    "\n",
    "df.to_csv(file_2_write, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=10, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  copy data frame to classification working data frame\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# ... data set with text categorical target values \n",
    "\n",
    "df_data_channel = df.copy()\n",
    "del df_data_channel['data_channel_n']\n",
    "\n",
    "# ... data set with integer categorical target values \n",
    "\n",
    "df_data_channel_n = df.copy()\n",
    "del df_data_channel_n['data_channel']\n",
    "del df_data_channel_n['kw_max_max']\n",
    "del df_data_channel_n['kw_avg_max']\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  separate X and y matrices \n",
    "# ...\n",
    "# ...  convert to numpy matrices by calling 'values' on the pandas data frames\n",
    "# ...  they are now simple matrices for compatibility with scikit-learn\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "if 'data_channel' in df_data_channel:\n",
    "    y = df_data_channel['data_channel'].values         # set 'data_channel' as dependent\n",
    "    del df_data_channel['data_channel']                # remove from dataset\n",
    "    X = df_data_channel.values                         # use everything else for independent EVs\n",
    "\n",
    "if 'data_channel_n' in df_data_channel_n:\n",
    "    y_n = df_data_channel_n['data_channel_n'].values    # set 'data_channel' as dependent\n",
    "    del df_data_channel_n['data_channel_n']             # remove from dataset\n",
    "    X_n = df_data_channel_n.values                      # use everything else for independent EVs\n",
    "\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  dataframe in which to record results of model metrics\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "results_table_labels = ['n_features', 'n_estimate', 'process_time', 'accuracy', 'recall', 'precision', 'f1_score']\n",
    "df_results = pd.DataFrame(columns = results_table_labels)\n",
    "\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  setup cross-validation in sklearn\n",
    "# ...\n",
    "# ...  split into training and test sets\n",
    "# ....  --> 10 folds\n",
    "# ...   --> 80% / 20% training / test\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "num_cv_iterations = 10\n",
    "\n",
    "num_instances = len(y)\n",
    "\n",
    "cv_object = ShuffleSplit(n_splits = num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ...  run through the cross validation loop and set the training and testing\n",
    "# ...  variable for one single iteration\n",
    "# ...\n",
    "# ...  --> this method is memory-user, but easier to follow what is being done \n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X, y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]    \n",
    "    X_test  = X[test_indices]\n",
    "    y_test  = y[test_indices]\n",
    "    \n",
    "    \n",
    "for train_indices, test_indices in cv_object.split(X_n, y_n): \n",
    "    X_train_n = X_n[train_indices]\n",
    "    y_train_n = y_n[train_indices]    \n",
    "    X_test_n  = X_n[test_indices]\n",
    "    y_test_n  = y_n[test_indices]\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# ... scale attributes by the training set\n",
    "# ... - normalize features based on mean & stdev of each column\n",
    "# ... - do not use the testing data\n",
    "# ... - use what was last stored in the variables: X_train, y_train, X_test, y_test\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)                        # scale for each column for (0,1) mean, std\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled  = scl_obj.transform(X_test) \n",
    "\n",
    "\n",
    "scl_obj.fit(X_train_n)                        # scale for each column for (0,1) mean, std\n",
    "    \n",
    "X_train_n_scaled = scl_obj.transform(X_train_n) # apply to training\n",
    "X_test_n_scaled  = scl_obj.transform(X_test_n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set required variables for model comparison\n",
    "comparison_tbl = pd.DataFrame(columns = ['Model Name','Accuracy','Precision','Recall','FScore','Processing Time'])\n",
    "i_index=[]\n",
    "i_index = 0\n",
    "\n",
    "# preparation for cross validation and model comparison, each classifier is appended once model is fit\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \tModeling and Evaluation 3\t\n",
    "# TODO ************** Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "\n",
    "### a. Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "          random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7621\n",
      "precision 0.7852\n",
      "recall 0.7621\n",
      "f1_score 0.7711\n",
      "confusion matrix\n",
      " [[ 993   16   57   18  145   46   15]\n",
      " [  19 1065   24  166   53   18   32]\n",
      " [  27    8  239   14   31   91   11]\n",
      " [   6  135   40  981   30    7   12]\n",
      " [  65   35   45   38  222   25   26]\n",
      " [  38   15  238    4   44 1088   60]\n",
      " [  18   50   30   18   59   57 1455]]\n",
      "process time 2.0875\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tic = time.clock()\n",
    "\n",
    "#basic multiclass LR\n",
    "lr_model1 = LogisticRegression(class_weight='balanced', multi_class='multinomial', solver='lbfgs')\n",
    "lr_model1.fit(X_train_scaled, y_train)  # train object\n",
    "y_hat = lr_model1.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "# calculate statistics\n",
    "accuracy = '{0:.4f}'.format(metrics.accuracy_score(y_test, y_hat))\n",
    "precision = '{0:.4f}'.format(metrics.precision_score(y_test, y_hat,average='weighted'))\n",
    "recall = '{0:.4f}'.format(metrics.recall_score(y_test, y_hat,average='weighted'))\n",
    "f1_score = '{0:.4f}'.format(metrics.f1_score(y_test, y_hat,average='weighted'))\n",
    "toc =  time.clock()\n",
    "exetime = '{0:.4f}'.format(toc-tic)\n",
    "\n",
    "# print statistics\n",
    "print(\"accuracy\",accuracy )\n",
    "print(\"precision\",precision )\n",
    "print(\"recall\",recall )\n",
    "print(\"f1_score\",f1_score )\n",
    "print(\"confusion matrix\\n\", conf(y_test, y_hat))\n",
    "print('process time',exetime)\n",
    "print(\"\\n\")\n",
    "\n",
    "# save statistics for model comparison\n",
    "raw_data = {\n",
    "    'Model Name':'Logistic Regression',\n",
    "    'Accuracy':accuracy,\n",
    "    'Precision':precision,\n",
    "    'Recall':recall,\n",
    "    'FScore':f1_score,\n",
    "    'Processing Time': exetime\n",
    "}\n",
    "df = pd.DataFrame(raw_data,columns = ['Model Name','Accuracy','Precision','Recall','FScore','Processing Time'],index=[i_index+1])\n",
    "comparison_tbl = comparison_tbl.append(df)\n",
    "\n",
    "#append model classifier for cross-validation\n",
    "models.append(('Logistic Regression',lr_model1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7387\n",
      "precision 0.7408\n",
      "recall 0.7387\n",
      "f1_score 0.7397\n",
      "confusion matrix\n",
      " [[1047   22   30   11   86   59   35]\n",
      " [  22 1028   22  164   51   44   46]\n",
      " [  33   23  147   22   27  153   16]\n",
      " [  21  193   35  912   17    5   28]\n",
      " [  58   45   23   34  210   37   49]\n",
      " [  52   30  160   13   42 1102   88]\n",
      " [  38   46   17   23   47  105 1411]]\n",
      "process time 2.2462\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "tic = time.clock()\n",
    "\n",
    "# train and fit\n",
    "DTclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "DTclassifier.fit(X_train, y_train)\n",
    "y_predDT = DTclassifier.predict(X_test)\n",
    "\n",
    "# calculate statistics\n",
    "accuracy = '{0:.4f}'.format(metrics.accuracy_score(y_test, y_predDT))\n",
    "precision = '{0:.4f}'.format(metrics.precision_score(y_test, y_predDT,average='weighted'))\n",
    "recall = '{0:.4f}'.format(metrics.recall_score(y_test, y_predDT,average='weighted'))\n",
    "f1_score = '{0:.4f}'.format(metrics.f1_score(y_test, y_predDT,average='weighted'))\n",
    "toc =  time.clock()\n",
    "exetime = '{0:.4f}'.format(toc-tic)\n",
    "\n",
    "# print statistics\n",
    "print(\"accuracy\",accuracy )\n",
    "print(\"precision\",precision )\n",
    "print(\"recall\",recall )\n",
    "print(\"f1_score\",f1_score )\n",
    "print(\"confusion matrix\\n\", confusion_matrix(y_test, y_predDT))\n",
    "print('process time',exetime)\n",
    "print(\"\\n\")\n",
    "\n",
    "# save statistics for model comparison\n",
    "raw_data = {\n",
    "    'Model Name':'Decision Tree Classifier',\n",
    "    'Accuracy':accuracy,\n",
    "    'Precision':precision,\n",
    "    'Recall':recall,\n",
    "    'FScore':f1_score,\n",
    "    'Processing Time': exetime\n",
    "}\n",
    "df = pd.DataFrame(raw_data,columns = ['Model Name','Accuracy','Precision','Recall','FScore','Processing Time'],index=[i_index+1])\n",
    "comparison_tbl = comparison_tbl.append(df)\n",
    "\n",
    "#append model classifier for cross-validation\n",
    "models.append(('Decision Tree Classifier',DTclassifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8011\n",
      "precision 0.7915\n",
      "recall 0.8011\n",
      "f1_score 0.7888\n",
      "confusion matrix\n",
      " [[1150   19    6    9   27   61   18]\n",
      " [  30 1140    5  134    4   38   26]\n",
      " [  59   15  107   26    9  189   16]\n",
      " [  10  129   15 1028    1    9   19]\n",
      " [ 105   54   13   36  147   47   54]\n",
      " [  52   25   60    3    9 1279   59]\n",
      " [  32   57    8   24    8   57 1501]]\n",
      "process time 1.6218\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "tic = time.clock()\n",
    "# train and test \n",
    "RFclf = RandomForestClassifier(criterion = 'entropy', max_depth=50, n_estimators=10, n_jobs=-1)\n",
    "RFclf.fit(X_train, y_train)\n",
    "y_predRF = RFclf.predict(X_test)\n",
    "\n",
    "\n",
    "# calculate statistics\n",
    "accuracy = '{0:.4f}'.format(metrics.accuracy_score(y_test, y_predRF))\n",
    "precision = '{0:.4f}'.format(metrics.precision_score(y_test, y_predRF,average='weighted'))\n",
    "recall = '{0:.4f}'.format(metrics.recall_score(y_test, y_predRF,average='weighted'))\n",
    "f1_score = '{0:.4f}'.format(metrics.f1_score(y_test, y_predRF,average='weighted'))\n",
    "toc =  time.clock()\n",
    "exetime = '{0:.4f}'.format(toc-tic)\n",
    "\n",
    "# print statistics\n",
    "print(\"accuracy\",accuracy )\n",
    "print(\"precision\",precision )\n",
    "print(\"recall\",recall )\n",
    "print(\"f1_score\",f1_score )\n",
    "print(\"confusion matrix\\n\", confusion_matrix(y_test, y_predRF))\n",
    "print('process time',exetime)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# save statistics for model comparison\n",
    "raw_data = {\n",
    "    'Model Name':'Random Forest Classifier',\n",
    "    'Accuracy':accuracy,\n",
    "    'Precision':precision,\n",
    "    'Recall':recall,\n",
    "    'FScore':f1_score,\n",
    "    'Processing Time': exetime\n",
    "}\n",
    "df = pd.DataFrame(raw_data,columns = ['Model Name','Accuracy','Precision','Recall','FScore','Processing Time'],index=[i_index+1])\n",
    "comparison_tbl = comparison_tbl.append(df)\n",
    "\n",
    "#append model classifier for cross-validation\n",
    "models.append(('Random Forest Classifier',RFclf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Container object of 50 artists>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEDCAYAAADUT6SnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18ae42add68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (RFclf)\n",
    "plt.barh(range(len(RFclf.feature_importances_)), RFclf.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3: Naive Bayes ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'logsumexp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-a8f3aaf0670d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf_mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_prior\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclf_mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Preeti\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'logsumexp'"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf_mnb = MultinomialNB(alpha = 1.0, class_prior = None, fit_prior = True)\n",
    "\n",
    "clf_mnb.fit(X_train_n, y_train_n)\n",
    "\n",
    "print('\\n class counts') \n",
    "print (clf_mnb.class_count_)\n",
    "print('\\n class prior prob (log)')\n",
    "print (clf_mnb.class_log_prior_)\n",
    "\n",
    "print('\\n\\n\\n feature counts') \n",
    "print (clf_mnb.feature_count_)\n",
    "print('\\n feature prob (log)')\n",
    "print (clf_mnb.feature_log_prob_)\n",
    "\n",
    "y_hatm = clf_mnb.predict(X_test_n)\n",
    "    \n",
    "accm = mt.accuracy_score(y_test_n, y_hatm)\n",
    "    \n",
    "print('MultinomialNB NaÃ¯ve Bayes Accuracy :', accm) \n",
    "\n",
    "# ... confusion matrix\n",
    "\n",
    "cm_nb = confusion_matrix(y_test_n, y_hatm)\n",
    "print(cm_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\tModeling and Evaluation 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>FScore</th>\n",
       "      <th>Processing Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.7621</td>\n",
       "      <td>0.7852</td>\n",
       "      <td>0.7621</td>\n",
       "      <td>0.7711</td>\n",
       "      <td>2.0875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7387</td>\n",
       "      <td>0.7408</td>\n",
       "      <td>0.7387</td>\n",
       "      <td>0.7397</td>\n",
       "      <td>2.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8011</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>0.8011</td>\n",
       "      <td>0.7888</td>\n",
       "      <td>1.6218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Name  Accuracy  Precision  Recall  FScore  \\\n",
       "0       Logistic Regression    0.7621     0.7852  0.7621  0.7711   \n",
       "1  Decision Tree Classifier    0.7387     0.7408  0.7387  0.7397   \n",
       "2  Random Forest Classifier    0.8011     0.7915  0.8011  0.7888   \n",
       "\n",
       "   Processing Time  \n",
       "0           2.0875  \n",
       "1           2.2462  \n",
       "2           1.6218  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting acc, pre, recall, fscore and time to numeric values for plots\n",
    "comparison_tbl = comparison_tbl.reset_index(drop=True)\n",
    "comparison_tbl['Precision'] =pd.to_numeric(comparison_tbl['Precision'])\n",
    "comparison_tbl['Accuracy']= pd.to_numeric(comparison_tbl['Accuracy'])\n",
    "comparison_tbl['FScore']= pd.to_numeric(comparison_tbl['FScore'])\n",
    "comparison_tbl['Processing Time'] = pd.to_numeric(comparison_tbl['Processing Time'])\n",
    "comparison_tbl['Recall']= pd.to_numeric(comparison_tbl['Recall'])\n",
    "comparison_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18ae41664e0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18aeba9b630>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x18aeb7165f8>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x18aebab53c8>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFjCAYAAAA6vqL8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHGEPAIDDEfYAADjc/wN8fYmV7YNOcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18aeb7b7a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comparison_tbl.plot()\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "sns.set(style=\"ticks\")\n",
    "sns.barplot(data=comparison_tbl,y='Accuracy',x='Model Name')\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.763361 (0.004847)\n",
      "Decision Tree Classifier: 0.741162 (0.006028)\n",
      "Random Forest Classifier: 0.796752 (0.006188)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tcv_results = model_selection.cross_val_score(model,X_train_scaled, y_train, cv=10, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x18ae44aebe0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'boxes': [<matplotlib.lines.Line2D at 0x18aeb7e9630>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e51d0>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e0dd8>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x18aeb7e8d30>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e7e48>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e3cc0>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e3748>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7df198>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7de588>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x18aeb7e6780>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e12b0>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7dd5c0>],\n",
       " 'means': [],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x18aeb7e6e48>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e2a90>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7de0b8>],\n",
       " 'whiskers': [<matplotlib.lines.Line2D at 0x18aeb7e9358>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e8b00>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e5d68>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e4198>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e0f60>,\n",
       "  <matplotlib.lines.Line2D at 0x18aeb7e0278>]}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.text.Text at 0x18aeb7fa780>,\n",
       " <matplotlib.text.Text at 0x18aeb7f2908>,\n",
       " <matplotlib.text.Text at 0x18aeb7dc4e0>]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAF2CAYAAAB6cnEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyv0/4F7ws5EdkEwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18ae44ae4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# boxplot accuracy comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Cross-Validation Accuracy Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \tExceptional Work\t\n",
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'C': 100, 'tol': 0.001}\n",
      "-1.707 (+/-0.005) for {'C': 100, 'tol': 0.001}\n",
      "[-1.709344   -1.70980855 -1.6985925  -1.70435365 -1.71102994]\n",
      "-1.707 (+/-0.005) for {'C': 100, 'tol': 0.0001}\n",
      "[-1.709344   -1.70980855 -1.6985925  -1.70435365 -1.71102994]\n",
      "-1.707 (+/-0.005) for {'C': 1000, 'tol': 0.001}\n",
      "[-1.709344   -1.70980853 -1.6985925  -1.70435365 -1.7110587 ]\n",
      "-1.707 (+/-0.005) for {'C': 1000, 'tol': 0.0001}\n",
      "[-1.709344   -1.70980853 -1.6985925  -1.70435365 -1.7110587 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "params = {'C':[100, 1000], 'tol': [0.001, 0.0001]}\n",
    "log_reg = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "clf = GridSearchCV(log_reg, params, scoring='log_loss', refit='True', n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"best params: \" + str(clf.best_params_))\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "  print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std(), params))\n",
    "  print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO list\n",
    "EDA for data_channel predition\n",
    "\n",
    "calculating recall and precision from confusion matrix. generic function for all models. \n",
    "\n",
    "cross validation for all models.\n",
    "\n",
    "Naive Bayes is throwing import error of sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rubrik\n",
    "### 1\tData Preparation Part 1\t\n",
    "10\t\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "### 2\tData Preparation Part 2\n",
    "5\t\n",
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "###  Modeling and Evaluation 1\t\n",
    "10\tChoose and explain your evaluation metrics that you will use (i.e., accuracy,precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "### \tModeling and Evaluation 2\t\n",
    "10\t\n",
    "Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "\n",
    "### \tModeling and Evaluation 3\t\n",
    "20\t\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "\n",
    "### \tModeling and Evaluation 4\t\n",
    "10\t\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "### \tModeling and Evaluation 5\t\n",
    "10\t\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniquesâ€”be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "\n",
    "### \tModeling and Evaluation 6\t\n",
    "10\t\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "\n",
    "### \tDeployment\t\n",
    "5\t\n",
    "How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?Â \n",
    "\n",
    "### \tExceptional Work\t\n",
    "10\t\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
